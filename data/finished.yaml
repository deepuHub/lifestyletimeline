- author: Sachin Tendulkar
  title: "Playing It My Way"
  finished: 2016-05-25
  rating: 4
- author:  Adam Geitgey @ Medium
  title: "Machine Learning is Fun! Part 1 - Part 8"  
  finished: 2017-11-18
  rating: 5
  quotes:
    - page: "4"
      content: "Modern Face Recognition with Deep Learning:- Machine learning people call the 128 measurements of each face an embedding. Encoding our face image This process of training a convolutional neural network to output face embeddings requires a lot of data and computer power. Even with an expensive NVidia Telsa video card, it takes about 24 hours of continuous training to get good accuracy. But once the network has been trained, it can generate measurements for any face, even ones it has never seen before! So this step only needs to be done once. Lucky for us, the fine folks at OpenFace already did this and they published several trained networks which we can directly use. Thanks Brandon Amos and team!"
    - page: "4"
      content: "Face Landmarks"
    - page: "4"
      content: "Histogram of Oreinted Gradients (HOG)"
    - page: "4"
      content: "Now that you know how this all works, here’s instructions from start-to-finish of how run this entire face recognition pipeline on your own computer:"
    - page: "5"
      content: "Language Translation with Deep Learning:- It turns out that over the past two years, deep learning has totally rewritten our approach to machine translation. Deep learning researchers who know almost nothing about language translation are throwing together relatively simple machine learning solutions that are beating the best expert-built language translation systems in the world. The technology behind this breakthrough is called sequence-to-sequence learning. It’s very powerful technique that be used to solve many kinds problems. After we see how it is used for translation, we’ll also learn how the exact same algorithm can be used to write AI chat bots and describe pictures. Let’s go!"
    - page: "5"
      content: "Parallel corpora: Building a statistics-based translation system requires lots of training data where the exact same text is translated into at least two languages. This double-translated text is called parallel corpora. In the same way that the Rosetta Stone was used by scientists in the 1800s to figure out Egyptian hieroglyphs from Greek, computers can use parallel corpora to guess how to convert text from one language to another."
    - page: "5"
      content: "Every time I fire a linguist, my accuracy goes up.— Frederick Jelinek"
    - page: "5"
      content: "RNN: But like most machine learning algorithms, neural networks are stateless. A recurrent neural network (or RNN for short) is a slightly tweaked version of a neural network where the previous state of the neural network is one of the inputs to the next calculation. This means that previous calculations change the results of future calculations!"
    - page: "5"
      content: "RNNs are useful any time you want to learn patterns in data. Because human language is just one big, complicated pattern, RNNs are increasingly used in many areas of natural language processing."
    - page: "5" 
      content: "Encoding: This is our encoding. It lets us represent something very complicated (a picture of a face) with something simple (128 numbers). Now comparing two different faces is much easier because we only have to compare these 128 numbers for each face instead of comparing full images. Guess what? We can do the same thing with sentences! We can come up with an encoding that represents every possible different sentence as a series of unique numbers. Because the RNN has a “memory” of each word that passed through it, the final encoding it calculates represents all the words in the sentence."
    - page: "5" 
      content: "Here’s where things get really cool! What if we took two RNNs and hooked them up end-to-end? The first RNN could generate the encoding that represents a sentence. Then the second RNN could take that encoding and just do the same logic in reverse to decode the original sentence again. Of course being able to encode and then decode the original sentence again isn’t very useful. But what if (and here’s the big idea!) we could train the second RNN to decode the sentence into Spanish instead of English? We could use our parallel corpora training data to train it to do that."
    - page: "6" 
      content: "Speech Recognition with Deep Learning:- Sound waves are one-dimensional. At every moment in time, they have a single value based on the height of the wave."
    - page: "6" 
      content: "To turn this sound wave into numbers, we just record of the height of the wave at equally-spaced points: This is called sampling. We are taking a reading thousands of times a second and recording a number representing the height of the sound wave at that point in time. That’s basically all an uncompressed .wav audio file is. CD Quality audio is sampled at 44.1khz (44,100 readings per second). But for speech recognition, a sampling rate of 16khz (16,000 samples per second) is enough to cover the frequency range of human speech."
    - page: "6"
      content: "But thanks to the Nyquist theorem, we know that we can use math to perfectly reconstruct the original sound wave from the spaced-out samples — as long as we sample at least twice as fast as the highest frequency we want to record."
    - page: "6"
      content: "Pre-processing our Sampled Sound Data: A spectrogram is cool because you can actually see musical notes and other pitch patterns in audio data. A neural network can find patterns in this kind of data more easily than raw sound waves. So this is the data representation we’ll actually feed into our neural network."
    - page: "6"
      content: "For a company like Google or Amazon, hundreds of thousands of hours of spoken audio recorded in real-life situations is gold. That’s the single biggest thing that separates their world-class speech recognition system from your hobby system. The whole point of putting Google Now! and Siri on every cell phone for free or selling $50 Alexa units that have no subscription fee is to get you to use them as much as possible. Every single thing you say into one of these systems is recorded forever and used as training data for future versions of speech recognition algorithms. That’s the whole game! Don’t believe me? If you have an Android phone with Google Now!, click here to listen to actual recordings of yourself saying every dumb thing you’ve ever said into it:"
    - page: "7"
      content: "Abusing Generative Adversarial Networks to Make 8-bit Pixel Art: Deep Convolutional Generative Adversarial Networks (or DCGANs for short). DCGANs are able to hallucinate original photo-realistic pictures by using a clever combination of two deep neural networks that compete with each other."
    - page: "7"
      content: "And the fake bills are being accepted as valid again! So now the Discriminator has to look again at the real dollar and find a new way to tell it apart from the fake one. This back-and-forth game between the Generator and the Discriminator continues thousands of times until both networks are experts. Eventually the Generator is producing near-perfect counterfeits and the Discriminator has turned into a Master Detective looking for the slightest mistakes."
    - page: "7"
      content: "Regurgitating training data is definitely something that can happen. By using a large training data set and not training too long, we can try to reduce the chance that this happens. But it’s a thorny issue and research on it continues."
    - page: "8"
      content: "How to Intentionally Trick Neural Networks: A Look into the Future of Hacking - Researchers have recently shown that you can train your own substitute neural network to mirror another neural network by probing it to see how it behaves. Then you can use your substitute neural network to generate hacked images that still often fool the original network! This is called a black-box attack."
    - page: "8"
      content: "How do we defend against this? The short answer is that no one is entirely sure yet. Preventing these kinds of attacks is still an on-going area of research. The best way to keep up with the latest developments is by reading the cleverhans blog maintained by Ian Goodfellow and Nicolas Papernot, two of the most influential researchers in this area."
    - page: "8"
      content: "If you simply create lots of hacked images and include them in your training data set going forward, that seems to make your neural network more resistant to these attacks. This is called Adversarial Training and is probably the most reasonable defense to consider adopting right now."
    - page: "8"
      content: "Since we don’t have any final answers yet, its worth thinking about the scenarios where you are using neural networks so that you can at least lessen the risk that this kind of attack would cause damage your business."
- author:  A. C. Bhaktivedanta Swami Prabhupada
  title: "The Science of Self-Realization"  
  finished: 2018-03-03
  rating: 5
  quotes:
    - page: "1"
      content: "Prof. Kotovsky: But by creating brahmanas from different social classes of society, you deny the old prescription of the Hindu scriptures. Srila Prabhupada: No, I establish it. Prof. Kotovsky: According to all scriptures—the Purtinas, etc.—everymember of one of these four classes of varnas has to be born within it. §rila Prabhupada: No, no, no, no. Prof. Kotovsky: That is the foundation of all the varnas- §rila Prabhupada: No, no. I am sorry. Prof. Kotovsky: The foundation of all the varnas- §rila PrabhupAda: You have spoken incorrectly. With great respect I be to submit that you are not speaking correctly. In the  Bhagavad-gita [4.13] it is stated, aitur-varnyarii mays srstath guna-karma-vibhagagah: These four orders of brahmanas, Icsatriyas, vain as, and gadras were created by Me according to quality and work. There is no mention of birth. Prof. Kotovsky: I agree withyou that this is the addition of later briihmanas who tried to perpetuate these qualities. Sala Prabhupada: That has killed theIndian culture. Otherwise there would have been no necessity of the division of part of India into Pakistan."
