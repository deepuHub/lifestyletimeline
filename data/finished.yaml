- author: Sachin Tendulkar
  title: "Playing It My Way"
  finished: 2016-05-25
  rating: 4
- author:  Adam Geitgey @ Medium
  title: "Machine Learning is Fun! Part 1 - Part 8"  
  finished: 2017-11-09
  rating: 5
  quotes:
    - page: "4"
      content: "Machine learning people call the 128 measurements of each face an embedding. Encoding our face image This process of training a convolutional neural network to output face embeddings requires a lot of data and computer power. Even with an expensive NVidia Telsa video card, it takes about 24 hours of continuous training to get good accuracy. But once the network has been trained, it can generate measurements for any face, even ones it has never seen before! So this step only needs to be done once. Lucky for us, the fine folks at OpenFace already did this and they published several trained networks which we can directly use. Thanks Brandon Amos and team!"
    - page: "4"
      content: "Face Landmarks"
    - page: "4"
      content: "Histogram of Oreinted Gradients (HOG)"
    - page: "4"
      content: "Now that you know how this all works, here’s instructions from start-to-finish of how run this entire face recognition pipeline on your own computer:"
    - page: "5"
      content: "It turns out that over the past two years, deep learning has totally rewritten our approach to machine translation. Deep learning researchers who know almost nothing about language translation are throwing together relatively simple machine learning solutions that are beating the best expert-built language translation systems in the world. The technology behind this breakthrough is called sequence-to-sequence learning. It’s very powerful technique that be used to solve many kinds problems. After we see how it is used for translation, we’ll also learn how the exact same algorithm can be used to write AI chat bots and describe pictures. Let’s go!"
    - page: "5"
      content: "Parallel corpora: Building a statistics-based translation system requires lots of training data where the exact same text is translated into at least two languages. This double-translated text is called parallel corpora. In the same way that the Rosetta Stone was used by scientists in the 1800s to figure out Egyptian hieroglyphs from Greek, computers can use parallel corpora to guess how to convert text from one language to another."
    - page: "5"
      content: "Every time I fire a linguist, my accuracy goes up.— Frederick Jelinek"
    - page: "5"
      content: "RNN: But like most machine learning algorithms, neural networks are stateless. A recurrent neural network (or RNN for short) is a slightly tweaked version of a neural network where the previous state of the neural network is one of the inputs to the next calculation. This means that previous calculations change the results of future calculations!"
    - page: "5"
      content: "RNNs are useful any time you want to learn patterns in data. Because human language is just one big, complicated pattern, RNNs are increasingly used in many areas of natural language processing."
    - page: "5" 
      content: "Encoding: This is our encoding. It lets us represent something very complicated (a picture of a face) with something simple (128 numbers). Now comparing two different faces is much easier because we only have to compare these 128 numbers for each face instead of comparing full images. Guess what? We can do the same thing with sentences! We can come up with an encoding that represents every possible different sentence as a series of unique numbers. Because the RNN has a “memory” of each word that passed through it, the final encoding it calculates represents all the words in the sentence."
    - page: "5" 
      content: "Here’s where things get really cool! What if we took two RNNs and hooked them up end-to-end? The first RNN could generate the encoding that represents a sentence. Then the second RNN could take that encoding and just do the same logic in reverse to decode the original sentence again. Of course being able to encode and then decode the original sentence again isn’t very useful. But what if (and here’s the big idea!) we could train the second RNN to decode the sentence into Spanish instead of English? We could use our parallel corpora training data to train it to do that."
    - page: "6" 
      content: ""
    - page: "6" 
      content: ""
    
