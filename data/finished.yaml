- author: Sachin Tendulkar
  title: "Playing It My Way"
  finished: 2016-05-25
  rating: 5
- author:  Adam Geitgey @ Medium
  title: "Machine Learning is Fun! Part 1 - Part 8"  
  finished: 2017-11-18
  rating: 5
  quotes:
    - page: "4"
      content: "Modern Face Recognition with Deep Learning:- Machine learning people call the 128 measurements of each face an embedding. Encoding our face image This process of training a convolutional neural network to output face embeddings requires a lot of data and computer power. Even with an expensive NVidia Telsa video card, it takes about 24 hours of continuous training to get good accuracy. But once the network has been trained, it can generate measurements for any face, even ones it has never seen before! So this step only needs to be done once. Lucky for us, the fine folks at OpenFace already did this and they published several trained networks which we can directly use. Thanks Brandon Amos and team!"
    - page: "4"
      content: "Face Landmarks"
    - page: "4"
      content: "Histogram of Oreinted Gradients (HOG)"
    - page: "4"
      content: "Now that you know how this all works, here’s instructions from start-to-finish of how run this entire face recognition pipeline on your own computer:"
    - page: "5"
      content: "Language Translation with Deep Learning:- It turns out that over the past two years, deep learning has totally rewritten our approach to machine translation. Deep learning researchers who know almost nothing about language translation are throwing together relatively simple machine learning solutions that are beating the best expert-built language translation systems in the world. The technology behind this breakthrough is called sequence-to-sequence learning. It’s very powerful technique that be used to solve many kinds problems. After we see how it is used for translation, we’ll also learn how the exact same algorithm can be used to write AI chat bots and describe pictures. Let’s go!"
    - page: "5"
      content: "Parallel corpora: Building a statistics-based translation system requires lots of training data where the exact same text is translated into at least two languages. This double-translated text is called parallel corpora. In the same way that the Rosetta Stone was used by scientists in the 1800s to figure out Egyptian hieroglyphs from Greek, computers can use parallel corpora to guess how to convert text from one language to another."
    - page: "5"
      content: "Every time I fire a linguist, my accuracy goes up.— Frederick Jelinek"
    - page: "5"
      content: "RNN: But like most machine learning algorithms, neural networks are stateless. A recurrent neural network (or RNN for short) is a slightly tweaked version of a neural network where the previous state of the neural network is one of the inputs to the next calculation. This means that previous calculations change the results of future calculations!"
    - page: "5"
      content: "RNNs are useful any time you want to learn patterns in data. Because human language is just one big, complicated pattern, RNNs are increasingly used in many areas of natural language processing."
    - page: "5" 
      content: "Encoding: This is our encoding. It lets us represent something very complicated (a picture of a face) with something simple (128 numbers). Now comparing two different faces is much easier because we only have to compare these 128 numbers for each face instead of comparing full images. Guess what? We can do the same thing with sentences! We can come up with an encoding that represents every possible different sentence as a series of unique numbers. Because the RNN has a “memory” of each word that passed through it, the final encoding it calculates represents all the words in the sentence."
    - page: "5" 
      content: "Here’s where things get really cool! What if we took two RNNs and hooked them up end-to-end? The first RNN could generate the encoding that represents a sentence. Then the second RNN could take that encoding and just do the same logic in reverse to decode the original sentence again. Of course being able to encode and then decode the original sentence again isn’t very useful. But what if (and here’s the big idea!) we could train the second RNN to decode the sentence into Spanish instead of English? We could use our parallel corpora training data to train it to do that."
    - page: "6" 
      content: "Speech Recognition with Deep Learning:- Sound waves are one-dimensional. At every moment in time, they have a single value based on the height of the wave."
    - page: "6" 
      content: "To turn this sound wave into numbers, we just record of the height of the wave at equally-spaced points: This is called sampling. We are taking a reading thousands of times a second and recording a number representing the height of the sound wave at that point in time. That’s basically all an uncompressed .wav audio file is. CD Quality audio is sampled at 44.1khz (44,100 readings per second). But for speech recognition, a sampling rate of 16khz (16,000 samples per second) is enough to cover the frequency range of human speech."
    - page: "6"
      content: "But thanks to the Nyquist theorem, we know that we can use math to perfectly reconstruct the original sound wave from the spaced-out samples — as long as we sample at least twice as fast as the highest frequency we want to record."
    - page: "6"
      content: "Pre-processing our Sampled Sound Data: A spectrogram is cool because you can actually see musical notes and other pitch patterns in audio data. A neural network can find patterns in this kind of data more easily than raw sound waves. So this is the data representation we’ll actually feed into our neural network."
    - page: "6"
      content: "For a company like Google or Amazon, hundreds of thousands of hours of spoken audio recorded in real-life situations is gold. That’s the single biggest thing that separates their world-class speech recognition system from your hobby system. The whole point of putting Google Now! and Siri on every cell phone for free or selling $50 Alexa units that have no subscription fee is to get you to use them as much as possible. Every single thing you say into one of these systems is recorded forever and used as training data for future versions of speech recognition algorithms. That’s the whole game! Don’t believe me? If you have an Android phone with Google Now!, click here to listen to actual recordings of yourself saying every dumb thing you’ve ever said into it:"
    - page: "7"
      content: "Abusing Generative Adversarial Networks to Make 8-bit Pixel Art: Deep Convolutional Generative Adversarial Networks (or DCGANs for short). DCGANs are able to hallucinate original photo-realistic pictures by using a clever combination of two deep neural networks that compete with each other."
    - page: "7"
      content: "And the fake bills are being accepted as valid again! So now the Discriminator has to look again at the real dollar and find a new way to tell it apart from the fake one. This back-and-forth game between the Generator and the Discriminator continues thousands of times until both networks are experts. Eventually the Generator is producing near-perfect counterfeits and the Discriminator has turned into a Master Detective looking for the slightest mistakes."
    - page: "7"
      content: "Regurgitating training data is definitely something that can happen. By using a large training data set and not training too long, we can try to reduce the chance that this happens. But it’s a thorny issue and research on it continues."
    - page: "8"
      content: "How to Intentionally Trick Neural Networks: A Look into the Future of Hacking - Researchers have recently shown that you can train your own substitute neural network to mirror another neural network by probing it to see how it behaves. Then you can use your substitute neural network to generate hacked images that still often fool the original network! This is called a black-box attack."
    - page: "8"
      content: "How do we defend against this? The short answer is that no one is entirely sure yet. Preventing these kinds of attacks is still an on-going area of research. The best way to keep up with the latest developments is by reading the cleverhans blog maintained by Ian Goodfellow and Nicolas Papernot, two of the most influential researchers in this area."
    - page: "8"
      content: "If you simply create lots of hacked images and include them in your training data set going forward, that seems to make your neural network more resistant to these attacks. This is called Adversarial Training and is probably the most reasonable defense to consider adopting right now."
    - page: "8"
      content: "Since we don’t have any final answers yet, its worth thinking about the scenarios where you are using neural networks so that you can at least lessen the risk that this kind of attack would cause damage your business."
- author:  A. C. Bhaktivedanta Swami Prabhupada
  title: "The Science of Self-Realization"  
  finished: 2018-03-04
  rating: 5
  quotes:
    - page: "1"
      content: "Prof. Kotovsky: But by creating brahmanas from different social classes of society, you deny the old prescription of the Hindu scriptures. Srila Prabhupada: No, I establish it. Prof. Kotovsky: According to all scriptures—the Purtinas, etc.—everymember of one of these four classes of varnas has to be born within it. §rila Prabhupada: No, no, no, no. Prof. Kotovsky: That is the foundation of all the varnas- §rila Prabhupada: No, no. I am sorry. Prof. Kotovsky: The foundation of all the varnas- §rila PrabhupAda: You have spoken incorrectly. With great respect I be to submit that you are not speaking correctly. In the  Bhagavad-gita [4.13] it is stated, aitur-varnyarii mays srstath guna-karma-vibhagagah: These four orders of brahmanas, Icsatriyas, vain as, and gadras were created by Me according to quality and work. There is no mention of birth. Prof. Kotovsky: I agree withyou that this is the addition of later briihmanas who tried to perpetuate these qualities. Sala Prabhupada: That has killed theIndian culture. Otherwise there would have been no necessity of the division of part of India into Pakistan."
    - page: "2"
      content: "It is not a religious faith. Religious faith you can -c-han-g—e, but real dharma you cannot change. Try to understand Krsna. In the Bhagavad-gita [18.66] He says, sarva-dharman parityajya mam ekath garanath vraja: Give up all other forms of religion and just surrender to Me. That is real knowl-edge—to surrender to the Supreme. You or I—anyone—is surrendered to someone. That is a fact. Our life is by surrender, is it not? Do you disagree with this point? Prof. Kotovsky: To some extent you surrender. §rila Prabhupacla: Yes, to the full extent. Prof. Kotovsky: You have to surrender to the society, for instance. To the whole people. §rila Prabhupacla: Yes, to the whole people, or to the state or to the king or the government or whatever you say. This surrender must be there. Prof. Kotovsky: The only difficulty is that we cannot half surrender to a government or a king. The principal difference is of surrender to a king, to a person, or to the society. §rila Prabhupacla: No, that is only a change of color. But the principle of surrender is there. Whether you surrender to monarchy, demoracy, ar-istocracy, or dictatorship, you have to surrender; that is a fact. cWithout surrender there is no life. It is not possible. So we are educating people to surrender to the Supreme, wherefrom you get all protection, just as Krsna says (sarva-dharman parityajya mam ekam §arahath vraja). No one can say, No, I am not surrendered to anyone. Not a single person. The difference is where he surrenders. The ultimate surrendering object is Krsna. There-to in the Bhagavad-gita [7.19] Krsna says, bahanartz janmanam ante frianavcin re  math prapadyate: After surrendering to so many things birth after birth, when one is factually wise he surrenders unto Me. Vasudevait sarvam iti sa mahatma sudurlabhah: Such a mahatma is very rare. Prof. Kotovsky: But at the same time it seems to me that surrender is to be accompanied by revolt. The history of mankind has proved that mankind has developed only by revolt against some kind of surrender the medieval age there was the French Revolution. It was revolt against  surrender. But this revolution itself was surrender to the rank and file of the people. You are agreed? Srila Prabhupada: Yes. Prof. Kotovsky: So it is not enough to come to a full stop. Surrender is to be accompanied with revolt against some and surrender to other people,. Prabhupada: But the surrender will be fully stopped when it is surrender to Krsna. . .  Prof. Kotovsky: Ah, ah. Prabhupicia: That is full stop—no more surrender. Any other sur-render you have to change by revolution. But when you come to Krsna, then it is sufficient. You are satisfied. I'll give you an example: a child is crying, and people move him from one lap to another. Oh, he does not stop. But as soon as the baby comes to the lap of his mother—Prof. Kotovsky: It stops. Srila Prabhupada: Yes, full satisfaction. So this surrender, these changes, will go on in different categories. But the sum total of all this surrender is surrender to maya. Therefore, in the Bhagavad-gita it is said that this sur-render, neglecting Krsna, is all maya. Either you surrender to this or to that, but final surrender is surrender to Krsna; then you will be happy. The process of surrender is there, but surrender to Krsna keeps one quite satisfied, transcendentally."
    - page: "3"
      content: "Srila Prabhupada: No, no. This is organized by the Englishmen and Americans. The Indian communities in London and San Francisco are trying to become—you know the word? Sahib? Prof. Kotovsky: [Laughs.] Westernized. [They both laugh.] A very great social anthropologist at the university has written something very inter-esting. He says there are two processes—the process of Westernization among bra-hirianas, mainly the upper class, and the process called Sanskritization, which is the process of adopting brahmana rituals, etc., by so-called lower classes, even untouchables. It is a very interesting process in India just now. But India's position, unfortunately, is problematic. Srila Prabhupada: The difficulty is that India is nowhere. They are trying to imitate Western life, but from a materialistic or technical point of view, they are one hundred years back. Prof. Kotovsky: Yes, that is right. But what to do for India? Srila Prabhupada: There is one thing I am experiencing. If India's spiri-tual asset is distributed, that will increase India's honor. Because every-where I go, people still adore Indian culture. If this treasure-house of India's spiritual knowledge is properly distributed, at least people outside of India will understand that they are getting something from India. Prof. Kotovsky: Of course, you're right. The Indian cultural heritage is to be made known everywhere. But at the same time, in what way would this benefit the Indian masses themselves? They are sitting in India, and they have nothing to gain from the spreading of the Indian cultural heritage all over the world. Indian villages have to have fertilizers, tractors, etc. Srila Prabhupada: Yes, we do not object to that."
    - page: "4"
      content: "Brahmananda Swami: Knowledge means that one must be able to dem-onstrate his theory. They should be able to show in their laboratories that life is caused by a combination of chemicals. Snla Prabhupada: Yes, the scientific method means first observation, then hypothesis, and then demonstration. But these scientists cannot demon-strate their hypothesis. They simply observe and then speak nonsense. Scientists say that the chemicals are the cause of life."
    - page: "5"
      content: "'BUILD YOUR NATIONS ON THE SPIRITUAL PLATFORM' Srila Prabhupada addresses students, faculty, and government officials at the University of Nairobi in September 1972: 'You are trying to develop yourselves so please develop spiritually, for spiritual development is sound development' Don't imitate the Americans and Europeans, who are living like cats an'd dogs. . . . The atomic bomb is already there, and as soon as the next war breaks out, all their skyscrapers and everything else will be finished. Try to understand this from the real viewpoint of human life, the spiritual viewpoint.' Ladies and gentlemen, thank you very much for kindly coming here to participate in this meeting for spreading Ki-sna consciousness. The Krsna consciousness movement is trying to bring human society to the point where everyone's life can become successful. The subject today is the real meaning of human life. We are trying to instruct the entire world about this meaning. Human life is attained after many, many millions of years species of life according to the Padma Purana. Life began with the aquatics, for we can understand from Vedic literature that at the beginning of creation the entire planet was merged in water. This material world is composed of five gross elements—earth, water, fire, air, and ether. Besides these there are three subtle elements—mind, intelligence, and ego. Behind these curtains is the spirit soul, which is covered by these eight elements. This informa-tion is given in the Bhagavad-gita. Human beings are not the only living entities to have a spirit soul. We are all spirit souls—beasts, birds, reptiles, insects, trees, plants, aquatics, and so on. The spirit soul is simply covered by different dresses, just as some of you are dressed in white clothes, some in green, some in red, etc. But we are not concerned with the dress; we are concerned with you as spirit soul. Thus it is said in the Bhagavad-gita [5.18]."
    - page: "6"
      content: "REACHING LIFE'S PERFECTION 261 Kalau means 'in this age.' Nasty eva, nasty eva, nasty eva-three times nasty eva. Eva means 'certainly,' and nasti means 'not.' 'Certainly not, certainly not, certainly not.' What is that 'certainly not'? One cannot realize oneself by karma. That is the first 'certainly not.' One cannot re-alize oneself by jnana. That is the second 'certainly not.' One cannot realize oneself by yoga. Certainly not. Kalau. Kalau means 'in this age.' Kalau nasty eva nasty eva nasty eva gatir anyatha. In this age one certainly cannot achieve success by any of these three methods. Then what is the recommended process? Harer nama harer nama harer namaiva kevalam-"
    - page: "6"
      content: "In Vedic scriptures such as the Narada-paricartitra and the Srimad Bhfigavatam. These two books, and the Bhagavad-gita, are very authentic scriptures meant for devotees. Caitanya Mahaprabhu quotes from a versa in the Mirada-pan-carat/7r hrsikeija hrsike§a-sevanarh bhaktir ucyate. This i the definition of pure devotional service. Hrsikena hrsikega-sevanam. Hrsikena means 'by one's senses.' We have to engage our senses; it is not that we engage only our minds. If someone says, 'I am always thinking of Krsna' that is not pure devotional service. Meditation is thinking, but no one thinks of Krsna; they think of void or something impersonal. If someone is thinking of Krsna or Narayana or Visnu, as prescribed in the Vedic scriptures, that is real yoga; yoga meditation means to focus one's mind upon the Supersoul. The Supersoul is the representation of Krsna in the form of four-handed Narayana. Even Patatijali, an authority on the yoga system, prescribes meditation on Visnu. But just as people are manu-facturing bogus religious processes, the so-called yogis of today have manufactured their own way of thinking of something void. But the Narada-paficartitra says, hrsikena hrsike§a-sevanam: one must engage not only one's mind but one's senses. Engage the senses in the service of the master of the senses. These three Sanskrit words are very significant. Hrsikena means 'the Lord of the senses.' So bhakti-yoga means to serve with the senses the Lord of the senses. The Lord of the senses is Krsna. We should always remember that we have our senses because we wanted to enjoy this material world, and therefore the Lord has given us a particular set of senses for our enjoyment. "
    - page: "6"
      content: "REACHING LIFE'S PERFECTION 277 Prahlada Maharaja was offered whatever he liked, he had only to ask for it, but he said, 'My Lord, I am Your eternal servant. It is my duty to serve You, so how can I accept any benefit from it? Then I would not be Your servant; I would be a merchant.' He replied in that way, and that is the sign of a pure person. Krsna is so kind that He fulfills all the desires of a devotee, even if he wants material benedictions. If at the bot-tom of the devotee's heart there is some desire, He also fulfills that. He is so kind. But the sublime position of bhakti-yoga, or devotional service, is that a pure devotee refuses to accept the various kinds of liberation, even if offered by the Supreme Lord. If one has material desires or motives within himself, and for fulfill-ment of such desires he engages himself in devotional service, the result will be that he will never get pure love of God."
    - page: "6"
      content: "Rupa Gosvami was very glad that Krsna had sent so many things and that he could now prepare a feast. He pi pared a feast and then invited his elder brother. When Sanatana Gosvami came, he was astonished. 'How have yc secured such things? You have prepared such a nice feast in this fore5 How is it possible?' So Riipa Gosvami explained, 'In the morning I desired for it, and b chance Krsna sent me all these things. A nice girl came, and she offere it to me.' He was describing the girl: 'A very nice girl.' Then Sanatana said, 'This nice girl is Radharani. You have taken servic from Radharani, the Lord's eternal consort. It is a great blunder.' That i their philosophy. They would not accept service from the Lord. They sim ply wanted to render service. But Krsna is so clever that He also wants tc serve His devotee. He looks for an opportunity to serve His devotee. Thi! is spiritual competition. A pure devotee does not want anything from Krsna; he simply wants to serve Him. And Krsna also looks for the oppor-tunity to serve His devotee. Krsna is always as anxious to please His devo-tee as the devotee is to please Him. This is the transcendental world. On the absolute plane, there is no exploitation. Everyone wants to serve; no one wants to take service. In the transcendental world, everyone wants to give service. You want to give service to me, and I want to give service to you. This is such a nice attitude. This material world means that I want to pickpocket you, and you want to pickpocket me. That's all. This is the material world. We should try to understand it. In the material world, everyone wants to exploit his friend, his father, his mother, everyone. But in the transcendental world, every-e wants to serve. Everyone has Krsna as the central point of serving."
    - page: "6"
      content: "A pure devotee's position is that he has no enemy because he is not envious. A pure devo-tee is always open to everyone, and he does not discriminate that this person can be allowed to chant Hare Krsna and that person should not be allowed. On the material platform, which is a platform of dualism, there are differences between high and low, man and woman, and this or that, but on the spiritual platform there are no such distinctions. The pure devotee, seeing everything with an equal mind, is therefore nonenvious. Because he is nonenvious, he is worshipable. Indeed, it may even be said that a person is worshipable simply if he is nonenvious, for it is only possible to be nonenvious on the spiritual platform. This is also the ver-dict of the Bhagavad-gita [5.18-19]: 'vidya-vinaya-sampanne brahmane gavi hastini guni caiva §vapake ca panclitalj sama-darginah ihaiva tair jitatj sargo yesarh samye sthitarh manalj nirdosarii hi samarh brahma tasmad brahmani to sthiteilj' The humble sages, by virtue of true knowledge, see with equal vision a learned and gentle brahmana, a cow, an elephant, a dog, and a dog-eater. Those whose minds are established in sameness and equanimity have already conquered the conditions of birth and death. They are flawless like Brahman, and thus they are already situated in Brahman.'"
    - page: "6"
      content: "Canakya Pandita even said that a rich man and a learned man cannot be compared, for a rich man may be hon-ored in his own country or on his own planet, but a learned man, a devotee of God, is honored wherever he goes."
    - page: "6"
      content: "Therefore Krsna says that worship of His devotee is even more valuable than worship of Him-self. The devotee is actually able to deliver Krsna, for he knows the science of Krsna consciousness, the science of hearing Krsna's words, eating krsna-prasaadam, and enjoying Krsna."
    - page: "6"
      content: "The underlying principle of devotional service is unalloyed love for Krsna. Regardless of the position of a particular devotee—as friend, ser-vant, parent, or lover of Krsna—his service is unconditional, for Krsna consciousness is not dependent on any material condition. It is transcen-dental and has nothing to do with the modes of material nature. A devo-tee is not afraid to go anywhere, and because of this he sees all material conditions as equal. In the world we may say that this is a good place to be and that is a bad place, but, as pointed out before, the devotee is not subject to these mental concoctions. For him the basic principle of ma-terial existence is bad, for material existence means forgetfulness of Krsna."
    - page: "7"
      content: "REACHING LIFE'S PERFECTION 287 One who realizes that God is full in six opulences actually begins rendering service. As soon as one becomes aware of the greatness of Krsna and understands Krsna's superiority, his service begins. The awareness of God's greatness increases when tran-scendental service is rendered. A person who serves the Lord in order to satisfy the senses of the Lord becomes satisfied, because Krsna is the Supersoul and the individual living entity is His part and parcel. If He is satisfied, then the living entity is satisfied. If the stomach is satisfied, then all the parts of the body are satisfied, for they receive nourishment through the stomach. When one of my Godbrothers began to fan my Guru Maharaja on a very hot day, Guru Maharaja asked, 'Why are you fanning me all of a sudden?' The boy replied, 'Because if you are satisfied, we are all satisfied.' This is the formula—we should not try to satisfy our senses separately, but should try to satisfy Krsna's senses. Then naturally we will become satisfied."
    - page: "7"
      content: "As far as the individual soul is concerned, it is originally a part and parcel of this pleasure potency, of the reservoir of pleasure Himself. However, due to contact with material nature, the soul has forgotten its actual position and has become trapped in the evolutionary process of transmigration from one body to another. Thus one struggles hard for existence. Now we must extricate ourselves from the sufferings of the struggle, from the countless transmigrations that force us to suffer the miseries of birth, old age, disease, and death, and come to the point of our eternal life in Krsna consciousness. That eternal life is possible. If one tries his best in this human form of life, in his next life he will get a spiritual body. The spiritual body is already within the gross material body, but it will develop only as soon as one becomes free from the contamination of this material existence. That is the aim of human life and the actual self-interest of all people. Self-interest is actually realizing, 'I am part and parcel of God. I have to return to the kingdom of God and join with Him.' Just as we have a social life here, God has a social life in the spiritual king-dom, and we can join Him there. It is not that after finishing this body we become void. In the Bhagavad-gita [2.12] Krsna told Arjuna, 'Never was there a time when I did not exist, nor you, nor all these kings, nor in the future shall any of us cease to be.' Our existence is therefore eternal, and the changes of birth and death are simply the changes of the temporary material bodies."
    - page: "8"
      content: "AN INTRODUCTION TO ISKCON 309 The basic principle of the Hare Krsna way of life is 'simple living and high thinking.' Devotees of Krsna are encour-aged to use their time, energy, talents, and resources in devotional service to God, and not to hanker for selfish ambitions or pleasures, which result in frustration and anxiety. To cultivate their inherent spiritual qualities of compassion, truthful-ness, cleanliness, and austerity, devotees follow four regulative principles, which also help them control the insatiable urges of the mind and senses. These principles are: 1. No eating of meat, fish, or eggs. 2. No gambling. 3. No sex other than for procreation within marriage. 4. No intoxicants, including all recreational drugs, alcohol, tobacco, tea, and coffee. According to the Bhagavad-gita and other Vedic literature, indulgence in the above activities disrupts our physical, mental, and spiritual well-being and increases anxiety and conflict in society."
    - page: "9"
      content: "A Philosophy for Everyone The philosophy of the Hare Krsna movement (a monotheistic tradition) is summarized in the following eight points: 1. By sincerely cultivating the authentic spiritual science presented in the Bhagavad-gita and other Vedic scriptures, we can become free from anxiety and achieve a state of pure, unending, blissful consciousness. 2. Each of us is not the material body but an eternal spirit soul, part and parcel of God (Krsna). As such, we are all the eternal servants of Krsna and are interrelated through Him, our common father. 3. Krsna is the eternal, all-knowing, omnipresent, all-powerful, and all-attractive Personality of Godhead. He is the seed-giving father of all living beings and the sustaining energy of the universe. He is the source of all incarnations of God, including Lord Buddha and Lord Jesus Christ. 4. The Vedas are the oldest scriptures in the world. The essence of the Vedas is found in the Bhagavad-gita, a literal record of Krsna's words spoken five thousands years ago in India. The goal of Vedic knowledge—and of all religions—is to achieve love of God. 5. We can perfectly understand the knowledge of self-realization through the instructions of a genuine spiritual master—one who is free from selfish motives, who teaches the science of God explained in the Bhagavad-gita, and whose mind is firmly fixed in meditation on Krsna. 6. All that we eat should first be offered to Lord Krsna with a prayer. In this way Krsna accepts the offering and blesses it for our purification. 7. Rather than living in a self-centered way, we should act for the plea-sure of Lord Krsna. This is known as bhakti-yoga, the science of devotional service. 8. The most effective means for achieving God consciousness in this Age of Kali, or quarrel, is to chant the holy names of the Lord: Hare Krsna, Hare Krsna, Krsna Krsna, Hare Hare/ Hare Rama, Hare Rama, Rama Rama, Hare Hare."
- author:  Gene Kim, Patrick Debois, John Willis and Jez Humble
  title: "The DevOps Handbook: How to Create World-Class Agility, Reliability, and Security in Technology Organizations"  
  finished: 2019-02-18
  rating: 5
  quotes:
    - page: "2"
      content: "The First Way: The Principles of Flow In the technology value stream, work typically flows from Development to Operations, the functional areas between our business and our customers. The First Way requires the fast and smooth flow of work from Development to Operations, to deliver value to customers quickly. We optimize for this global goal instead of local goals, such as Development feature completion rates, test find/fix ratios, or Ops availability measures. Limiting WIP also makes it easier to see problems that prevent the completion of work.[†] For instance, when we limit WIP, we find that we may have nothing to do because we are waiting on someone else. Although it may be tempting to start new work (i.e., “It’s better to be doing something than nothing'), a far better action would be to find something than nothing'), a far better action would be to find out what is causing the delay and help fix that problem. Bad multitasking often occurs when people are assigned to multiple projects, resulting in many prioritization problems. In other words, as David J. Andersen, author of Kanban: Successful Evolutionary Change for Your Technology Business, quipped, “Stop starting. Start finishing.'"
    - page: "2"
      content: "CONCLUSION Improving flow through the technology value stream is essential to achieving DevOps outcomes. We do this by making work visible, limiting WIP, reducing batch sizes and the number of handoffs, continually identifying and evaluating our constraints, and eliminating hardships in our daily work."
    - page: "3"
      content: "When failures and accidents occur, we treat them as opportunities for learning, as opposed to a cause for punishment and blame."
    - page: "3"
      content: "As Elisabeth Hendrickson, VP of Engineering at Pivotal Software, Inc. and author of Explore It!: Reduce Risk and Increase Confidence with Exploratory Testing, said, 'When I headed up quality engineering, I described my job as 'creating feedback cycles.' Feedback is critical because it is what allows us to steer. We must constantly validate between customer needs, our intentions and our implementations. Testing is merely one sort of feedback.'"
    - page: "3"
      content: "Swarming is necessary for the following reasons: It prevents the problem from progressing downstream, where the cost and effort to repair it increases exponentially and technical debt is allowed to accumulate. It prevents the work center from starting new work, which will likely introduce new errors into the system. If the problem is not addressed, the work center could potentially have the same problem in the next operation (e.g., fifty-five seconds later), requiring more fixes and work. See Appendix 6. This practice of swarming seems contrary to common management practice, as we are deliberately allowing a local problem to disrupt operations globally. However, swarming enables learning. Changes that pass our continuous build and integration tests are deployed into production, and any changes that cause any tests to fail trigger our Andon cord and are swarmed until resolved. Astonishingly, when the number of Andon cord pulls drop, plant managers will actually decrease the tolerances to get an increase in the number of Andon cord pulls in order to continue to enable more learnings and improvements and to detect ever-weaker failure signals."
    - page: "3"
      content: "Gary Gruver observes, 'It's impossible for a developer to learn anything when someone yells at them for something they broke six months ago—that's why we need to provide feedback to everyone as quickly as possible, in minutes, not months.'"
    - page: "3"
      content: "CONCLUSION Creating fast feedback is critical to achieving quality, reliability, and safety in the technology value stream. We do this by seeing problems as they occur, swarming and solving problems to build new knowledge, pushing quality closer to the source, and continually optimizing for downstream work centers. The specific practices that enable fast flow in the DevOps value stream are presented in Part IV."
    - page: "4"
      content: "Moreover, any local learnings are rapidly turned into global improvements, so that new techniques and practices can be used by the entire organization. We reserve time for the improvement of daily work and to further accelerate and ensure learning. We consistently introduce stress into our systems to force continual improvement. We even simulate and inject failures in our production services under controlled conditions to increase our resilience."
    - page: "4"
      content: "'Responses to incidents and accidents that are seen as unjust can impede safety investigations, promote fear rather than mindfulness in people who do safety-critical work, make organizations more bureaucratic rather than more careful, and cultivate professional secrecy, evasion, and self-protection.' 'By removing blame, you remove fear; by removing fear, you enable honesty; and honesty enables prevention.'"
    - page: "4"
      content: "This is why Mike Orzen, author of Lean IT, observed, 'Even more important than daily work is the improvement of daily work.'"
    - page: "4"
      content: "A remarkable example of turning local knowledge into global knowledge is the US Navy's Nuclear Power Propulsion Program (also known as 'NR' for 'Naval Reactors'), which has over 5,700 reactor-years of operation without a single reactor-related casualty or escape of radiation. The NR is known for their intense commitment to scripted procedures and standardized work, and the need for incident reports for any departure from procedure or normal operations to accumulate learnings, no matter how minor the failure signal—they constantly update procedures and system designs based on these learnings. By relentless and constant experimentation in their daily work, they were able to continually increase capacity, often without adding any new equipment or hiring more people."
    - page: "4"
      content: "This process of applying stress to increase resilience was named antifragility by author and risk analyst Nassim Nicholas Taleb."
    - page: "4"
      content: "However, there is significant evidence that shows greatness is not achieved by leaders making all the right decisions—instead, the leader's role is to create the conditions so their team can discover greatness in their daily work."
    - page: "4"
      content: "Mike Rother observes that he sees Toyota 'as an organization defined primarily by the unique behavior routines it continually teaches to all its members.'"
    - page: "4"
      content: "CONCLUSION The principles of the Third Way address the need for valuing organizational learning, enabling high trust and boundary-spanning between functions, accepting that failures will always occur in complex systems, and making it acceptable to talk about problems so we can create a safe system of work. It also requires institutionalizing the improvement of daily work, converting local learnings into global learnings that can be used by the entire organization, as well as continually injecting tension into our daily work."
    - page: "5"
      content: "Instead, they wanted to focus on very specific areas of the business so that they could experiment and learn. Their goal was to demonstrate early wins, which would give everyone confidence that these improvements could be replicated in other areas of the organization. How exactly that would be achieved was still unknown. They focused on three areas: the customer mobile application, their in-store restaurant systems, and their digital properties. Each of these areas had business goals that weren't being met; thus, they were more receptive to considering a different way of working."
    - page: "5"
      content: "In technology, a greenfield project is a new software project or initiative, likely in the early stages of planning or implementation, where we build our applications and infrastructure anew, with few constraints. Starting with a greenfield software project can be easier, especially if the project is already funded and a team is either being created or is already in place."
    - page: "5"
      content: "Greenfield DevOps projects are often pilots to demonstrate feasibility of public or private clouds, piloting deployment automation, and similar tools. Brownfield DevOps projects, these are existing products or services that are already serving customers and have potentially been in operation for years or even decades. Although many believe that DevOps is primarily for greenfield projects, DevOps has been used to successfully transform brownfield projects of all sorts. Indeed, one of the findings in the 2015 State of DevOps Report validated that the age of the application was not a significant predictor of performance; instead, what predicted performance was whether the application was architected (or could be re-architected) for testability and deployability."
    - page: "5"
      content: "In 2009, Etsy had thirty-five employees and was generating $87 million in revenue, but after they 'barely survived the holiday retail season,' they started transforming virtually every aspect of how the organization worked, eventually turning the company into one of the most admired DevOps organizations and set the stage for a successful 2015 IPO."
    - page: "5"
      content: "The practice of relying on a stabilization phase or hardening phase at the end of a project often has very poor outcomes, because it means problems are not being found and fixed as part of daily work and are left unaddressed, potentially snowballing into larger issues."
    - page: "5"
      content: "Within bimodal IT there are systems of record, the ERP-like systems that run our business (e.g., MRP, HR, financial reporting systems), where the correctness of the transactions and data are paramount; and systems of engagement, which are customer-facing or employee-facing systems, such as e-commerce systems and productivity applications."
    - page: "5"
      content: "Regardless of how we scope our initial effort, we must demonstrate early wins and broadcast our successes. We do this by breaking up our larger improvement goals into small, incremental steps. This not only creates our improvements faster, it also enables us to discover when we have made the wrong choice of value stream—by detecting our errors early, we can quickly back up and try again, making different decisions armed with our new learnings."
    - page: "5"
      content: "'Leading change requires courage, especially in corporate environments where people are scared and fight you. But if you start small, you really have nothing to fear. Any leader needs to be brave enough to allocate teams to do some calculated risk-taking.'"
    - page: "6"
      content: "Our first pass of documenting our value stream should only consist of high-level process blocks. Typically, even for complex value streams, groups can create a diagram with five to fifteen process blocks within a few hours. Each process block should include the lead time and process time for a work item to be processed, as well as the %C/A as measured by the downstream consumers of the output."
    - page: "6"
      content: "We will actively manage this technical debt by ensuring that we invest at least 20% of all Development and Operations cycles on refactoring, investing in automation work and architecture and non-functional requirements (NFRs, sometimes referred to as the 'ilities'), such as maintainability, manageability, scalability, reliability, testability, deployability, and security. Invest 20% of cycles on those that create positive, user-invisible value (Source: 'Machine Learning and Technical Debt with D. Sculley,' Software Engineering Daily podcast, November 17, 2015, http://softwareengineeringdaily.com/2015/11/17/machine-learning-and-technical-debt-with-d-sculley/ .)"
    - page: "6"
      content: "After the near-death experience of eBay in the late 1990s, Marty Cagan, author of Inspired: How To Create Products Customers Love, the seminal book on product design and management, codified Conway's Law in his Jargon File: 'The organization of the software and the organization of the software team will be congruent; commonly stated as 'if you have four groups working on a compiler, you'll get a 4-pass compiler.''"
    - page: "6"
      content: "ORGANIZATIONAL ARCHETYPES In the field of decision sciences, there are three primary types of organizational structures that inform how we design our DevOps value streams with Conway's Law in mind: functional, matrix, and market. They are defined by Dr. Roberto Fernandez"
    - page: "7"
      content: "Among many things, an ORM abstracts a database, enabling developers to do queries and data manipulation as if they were merely another object in the programming language. Popular ORMs include Hibernate for Java, SQLAlchemy for Python, and ActiveRecord for Ruby on Rails."
    - page: "7"
      content: "Sprouter, shorthand for 'stored procedure router,' Sprouter was one of many technologies used in development and production that Etsy eliminated as part of their transformation."
    - page: "7"
      content: "PROBLEMS OFTEN CAUSED BY OVERLY FUNCTIONAL ORIENTATION ('OPTIMIZING FOR COST') In traditional IT Operations organizations, we often use functional orientation to organize our teams by their specialties. We put the database administrators in one group, the network administrators in another, the server administrators in a third, and so forth. One of the most visible consequences of this is long lead times. The problem is exacerbated when each Operations functional area has to serve multiple value streams (i.e., multiple Development teams) who all compete for their scarce cycles. In order for Development teams to get their work done in a timely manner, we often have to escalate issues to a manager or director, and eventually to someone (usually an executive) who can finally prioritize the work against the global organizational goals instead of the functional silo goals. This decision must then get cascaded down into each of the functional areas to change the local priorities, and this, in turn, slows down other teams. When every team expedites their work, the net result is that every project ends up moving at the same slow crawl."
    - page: "7"
      content: "ENABLE MARKET-ORIENTED TEAMS ('OPTIMIZING FOR SPEED') Broadly speaking, to achieve DevOps outcomes, we need to reduce the effects of functional orientation ('optimizing for cost') and enable market orientation ('optimizing for speed') so we can have many small teams working safely and independently, quickly delivering value to the customer. This model has been adopted by Amazon and Netflix and is touted by Amazon as one of the primary reasons behind their ability to move fast even as they grow. For example, high performance with a functional-oriented and centralized Operations group is possible, as long as service teams get what they need from Operations reliably and quickly (ideally on demand) and vice-versa. Many of the most admired DevOps organizations retain functional orientation of Operations, including Etsy, Google, and GitHub."
    - page: "8"
      content: "However, we must remind everyone that improvement of daily work is more important than daily work itself, and that all teams must have dedicated capacity for this (e.g., reserving 20% of all cycles for improvement work, scheduling one day per week or one week per month, etc.). Without doing this, the productivity of the team will almost certainly grind to a halt under the weight of its own technical and process debt."
    - page: "8"
      content: "CONCLUSION Throughout this chapter, we explored ways to integrate Operations into the daily work of Development, and looked at how to make our work more visible to Operations. To accomplish this, we explored three broad strategies, including creating self-service capabilities to enable developers in service teams to be productive, embedding Ops engineers into the service teams, and assigning Ops liaisons to the service teams when embedding Ops engineers was not possible. Lastly, we described how Ops engineers can integrate with the Dev team through inclusion in their daily work, including daily standups, planning, and retrospectives."
    - page: "8"
      content: "Part III:The First Way—The Technical Practices Of Flow Continuous delivery includes creating the foundations of our automated deployment pipeline, ensuring that we have automated tests that constantly validate that we are in a deployable state, having developers integrate their code into trunk daily, and architecting our environments and code to enable low-risk releases."
    - page: "9"
      content: "we want developers to run production-like environments on their own workstations, created on demand and self-serviced. By doing this, developers can run and test their code in production-like environments as part of their daily work, providing early and constant feedback on the quality their work."
    - page: "9"
      content: "However, because delivering value to the customer requires both our code and the environments they run in, we need our environments in version control as well. In other words, version control is for everyone in our value stream, including QA, Operations, Infosec, as well as developers. By putting all production artifacts into version control, our version control repository enables us to repeatedly and reliably reproduce all components of our working software system—this includes our applications and production environment, as well as all of our pre-production environments."
    - page: "9"
      content: "In Puppet Labs' 2014 State of DevOps Report, the use of version control by Ops was the highest predictor of both IT performance and organizational performance. In fact, whether Ops used version control was a higher predictor for both IT performance and organizational performance than whether Dev used version control. But why does using version control for our environments predict IT and organizational performance better than using version control for our code? Because in almost all cases, there are orders of magnitude more configurable settings in our environment than in our code. Consequently, it is the environment that needs to be in version control the most."
    - page: "9"
      content: "Bill Baker, a distinguished engineer at Microsoft, quipped that we used to treat servers like pets: 'You name them and when they get sick, you nurse them back to health. [Now] servers are [treated] like cattle. You number them and when they get sick, you shoot them.'"
    - page: "9"
      content: "Bill Baker, a distinguished engineer at Microsoft, quipped that we used to treat servers like pets: 'You name them and when they get sick, you nurse them back to health. [Now] servers are [treated] like cattle. You number them and when they get sick, you shoot them.'"
    - page: "9"
      content: "We can rely on our automated configuration systems to ensure consistency (e.g., Puppet, Chef, Ansible, Salt, Bosh, etc.), or we can create new virtual machines or containers from our automated build mechanism and deploy them into production, destroying the old ones or taking them out of rotation. The latter pattern is what has become known as immutable infrastructure, where manual changes to the production environment are no longer allowed—the only way production changes can be made is to put the changes into version control and re-create the code and environments from scratch. By doing this, no variance is able to creep into production. To prevent uncontrolled configuration variances, we may disable remote logins to production servers or routinely kill and replace production instances."
    - page: "9"
      content: "CONCLUSION The fast flow of work from Development to Operations requires that anyone can get production-like environments on demand. By allowing developers to use production-like environments even at the earliest stages of a software project, we significantly reduce the risk of production problems later. This is one of many practices that demonstrate how Operations can make developers far more productive. We enforce the practice of developers running their code in production-like environments by incorporating it into the definition of 'done.' Furthermore, by putting all production artifacts into version control, we have a 'single source of truth' that allows us to re-create the entire production environment in a quick, repeatable, and documented way, using the same development practices for Operations work as we do for Development work. And by making production infrastructure easier to rebuild than to repair, we make resolving problems easier and faster, as well as making it easier to expand capacity."
    - page: "10"
      content: "Enable Fast and Reliable Automated Testing - without automated testing, the more code we write, the more time and money is required to test our code—in most cases, this is a totally unscalable business model for any technology organization."  
    - page: "10"
      content: "our deployment pipeline infrastructure becomes as foundational for our development processes as our version control infrastructure. Our deployment pipeline also stores the history of each code build, including information about which tests were performed on which build, which builds have been deployed to which environment, and what the test results were. In combination with the information in our version control history, we can quickly determine what caused our deployment pipeline to break and, likely, how to fix the error."  
    - page: "10"
      content: "Now that we have a working deployment pipeline infrastructure, we must create our continuous integration practices, which require three capabilities: A comprehensive and reliable set of automated tests that validate we are in a deployable state. A culture that 'stops the entire production line' when our validation tests fail. Developers working in small batches on trunk rather than long-lived feature branches."  
    - page: "10"
      content: "The aim of a unit test is to show that a single part of the application does what the programmer intends it to....The objective of acceptance tests is to prove that our application does what the customer meant it to, not that it works the way its programmers think it should."  
    - page: "10"
      content: "One of the most effective ways to ensure we have reliable automated testing, is to write those tests as part of our daily work, using techniques such as test-driven development (TDD) and acceptance test-driven development (ATDD). This is when we begin every change to the system by first writing an automated test that validates the expected behavior fails, and then we write the code to make the tests pass. This technique was developed by Kent Beck in the late 1990s as part of Extreme Programming, and has the following three steps: Ensure the tests fail. 'Write a test for the next bit of functionality you want to add.' Check in. Ensure the tests pass. 'Write the functional code until the test passes.'Check in. 'Refactor both new and old code to make it well structured.'Ensure the tests pass. Check in again."  
    - page: "10"
      content: "'Although testing can be automated, creating quality cannot. To have humans executing tests that should be automated is a waste of human potential.'"  
    - page: "10"
      content: "Many teams have created highly visible build lights that get mounted on a wall, indicating the current build status, or other fun ways of telling the team the build is broken, including lava lamps, playing a voice sample or song, klaxons, traffic lights, and so forth."  
    - page: "10"
      content: "CONCLUSION In this chapter, we have created a comprehensive set of automated tests to confirm that we have a green build that is still in a passing and deployable state. We have organized our test suites and testing activities into a deployment pipeline. We have also created the cultural norm of doing whatever it takes to get back into a green build state if someone introduces a change that breaks any of our automated tests. By doing this, we set the stage for implementing continuous integration, which allows many small teams to independently and safely develop, test, and deploy code into production, delivering value to customers."  
    - page: "11"
      content: "Enable and Practice Continuous Integration CONCLUSION Trunk-based development is likely the most controversial practice discussed in this book. Many engineers will not believe that it’s possible, even those that prefer working uninterrupted on a private branch without having to deal with other developers. However, the data from Puppet Labs’ 2015 State of DevOps Report is clear: trunk-based development predicts higher throughput and better stability, and even higher job satisfaction and lower rates of burnout."  
    - page: "12"
      content: "Automate and Enable Low-Risk Releases - In practice, the terms deployment and release are often used interchangeably. However, they are two distinct actions that serve two very different purposes: Deployment is the installation of a specified version of software to a given environment (e.g., deploying code into an integration test environment or deploying code into production). Specifically, a deployment may or may not be associated with a release of a feature to customers. Release is when we make a feature (or set of features) available to all our customers or a segment of customers (e.g., we enable the feature to be used by 5% of our customer base). Our code and environments should be architected in such a way that the release of functionality does not require changing our application code."  
    - page: "12"
      content: "There are two broad categories of release patterns we can use: Environment-based release patterns: This is where we have two or more environments that we deploy into, but only one environment is receiving live customer traffic (e.g., by configuring our load balancers). New code is deployed into a non-live environment, and the release is performed moving traffic to this environment. These are extremely powerful patterns, because they typically require little or no change to our applications. These patterns include blue-green deployments, canary releases, and cluster immune systems, all of which will be discussed shortly. Application-based release patterns: This is where we modify our application so that we can selectively release and expose specific application functionality by small configuration changes. For instance, we can implement feature flags that progressively expose new functionality in production to the development team, all internal employees, 1% of our customers, or, when we are confident that the release will operate as designed, our entire customer base. As discussed earlier, this enables a technique called dark launching, where we stage all the functionality to be launched in production and test it with production traffic before our release. For instance, we may invisibly test our new functionality with production traffic for weeks before our launch in order to expose problems so that they can be fixed before our actual launch."  
    - page: "12"
      content: "Decoupling deployments from our releases dramatically changes how we work. We no longer have to perform deployments in the middle of the night or on weekends to lower the risk of negatively impacting customers. Instead, we can do deployments during typical business hours, enabling Ops to finally have normal working hours, just like everyone else."  
    - page: "12"
      content: "The Blue-Green Deployment Pattern The simplest of the three patterns is called blue-green deployment. In this pattern, we have two production environments: blue and green. At any time, only one of these is serving customer traffic—in figure 20, the green environment is live. Figure 20: Blue-green deployment patterns (Source: Humble and North, Continuous Delivery, 261.) To release a new version of our service, we deploy to the inactive environment where we can perform our testing without interrupting the user experience. When we are confident that everything is functioning as designed, we execute our release by directing traffic to the blue environment. Thus, blue becomes live and green becomes staging. Roll back is performed by sending customer traffic back to the green environment."  
    - page: "12"
      content: "The canary release pattern automates the release process of promoting to successively larger and more critical environments as we confirm that the code is operating as designed."  
    - page: "12"
      content: "Implement Feature Toggles The primary way we enable application-based release patterns is by implementing feature toggles, which provide us with the mechanism to selectively enable and disable features without requiring a production code deployment. Feature toggles can also control which features are visible and available to specific user segments (e.g., internal employees, segments of customers)."  
    - page: "12"
      content: "As part of their dark launch process, every Facebook user session, which runs JavaScript in the user browser, had a test harness loaded into it—the chat UI elements were hidden, but the browser client would send invisible test chat messages to the back-end chat service that was already in production, enabling them to simulate production-like loads throughout the entire project, allowing them to find and fix performance problems long before the customer release."  
    - page: "12"
      content: "His updated definitions of continuous delivery and continuous deployment are as follows: When all developers are working in small batches on trunk, or everyone is working off trunk in short-lived feature branches that get merged to trunk regularly, and when trunk is always kept in a releasable state, and when we can release on demand at the push of a button during normal business hours, we are doing continuous delivery. Developers get fast feedback when they introduce any regression errors, which include defects, performance issues, security issues, usability issues, etc. When these issues are found, they are fixed immediately so that trunk is always deployable. In addition to the above, when we are deploying good builds into production on a regular basis through self-service (being deployed by Dev or by Ops)—which typically means that we are deploying to production at least once per day per developer, or perhaps even automatically deploying every change a developer commits—this is when we are engaging in continuous deployment. Defined this way, continuous delivery is the prerequisite for continuous deployment—just as continuous integration is a prerequisite for continuous delivery. Continuous deployment is likely applicable in the context of web services that are delivered online."  
    - page: "12"
      content: "CONCLUSION As the Facebook, Etsy, and CSG examples have shown, releases and deployments do not have to be high-risk, high-drama affairs that require tens or hundreds of engineers to work around the clock to complete. Instead, they can be made entirely routine and a part of everyone’s daily work. By doing this, we can reduce our deployment lead times from months to minutes, allowing our organizations to quickly deliver value to our customer without causing chaos and disruption. Furthermore, by having Dev and Ops work together, we can finally make Operations work humane."  
    - page: "13"
      content: "Architect for Low-Risk Releases - Lesson 1: When applied rigorously, strict service orientation is an excellent technique to achieve isolation; you achieve a level of ownership and control that was not seen before. Lesson 2: Prohibiting direct database access by clients makes performing scaling and reliability improvements to your service state possible without involving your clients. Lesson 3: Development and operational process greatly benefits from switching to service-orientation. The services model has been a key enabler in creating teams that can innovate quickly with a strong customer focus. Each service has a team associated with it, and that team is completely responsible for the service—from scoping out the functionality to architecting, building, and operating it."  
    - page: "13"
      content: "The extent to which applying these lessons enhances developer productivity and reliability is breathtaking. In 2011, Amazon was performing approximately fifteen thousands deployments per day. By 2015, they were performing nearly 136,000 deployments per day."  
    - page: "14"
      content: "Create Telemetry to Enable Seeing and Solving Problems - An event router responsible for storing our events and metrics: Figure 26: Monitoring framework"  
    - page: "14"
      content: "'Monitoring is so important that our monitoring systems need to be more available and scalable than the systems being monitored.'"  
    - page: "14"
      content: "'Every time NASA launches a rocket, it has millions of automated sensors reporting the status of every component of this valuable asset. And yet, we often don’t take the same care with software—we found that creating application and infrastructure telemetry to be one of the highest return investments we’ve made. In 2014, we created over one billion telemetry events per day, with over one hundred thousand code locations instrumented.'"  
    - page: "14"
      content: "In the applications we create and operate, every feature should be instrumented—if it was important enough for an engineer to implement, it is certainly important enough to generate enough production telemetry so that we can confirm that it is operating as designed and that the desired outcomes are being achieved."  
    - page: "14"
      content: "Whether a message should be ERROR or WARN, imagine being woken up at 4 a.m. Low printer toner is not an ERROR."  
    - page: "14"
      content: "we should ensure that all potentially significant application events generate logging entries, including those provided on this list assembled by Anton A. Chuvakin, a research VP at Gartner’s GTP Security and Risk Management group: Authentication/authorization decisions (including logoff) System and data access System and application changes (especially privileged changes) Data changes, such as adding, editing, or deleting data Invalid input (possible malicious injection, threats, etc.) Resources (RAM, disk, CPU, bandwidth, or any other resource that has hard or soft limits) Health and availability Startups and shutdowns Faults and errors Circuit breaker trips Delays Backup success/failure To make it easier to interpret and give meaning to all these log entries, we should (ideally) create logging hierarchical categories, such as for non-functional attributes (e.g., performance, security) and for attributes related to features (e.g., search, ranking)."  
    - page: "14"
      content: "By doing this, we can detect conditions that could indicate problems, such as if the performance test or our build takes twice as long as normal, allowing us to find and fix errors before they go into production."  
    - page: "14"
      content: "USE TELEMETRY TO GUIDE PROBLEM SOLVING As described in the beginning of this chapter, high performers use a disciplined approach to solving problems."  
    - page: "14"
      content: "'We designed StatsD to prevent any developer from saying, ‘It’s too much of a hassle to instrument my code.’ Now they can do it with one line of code. It was important to us that for a developer, adding production telemetry didn’t feel as difficult as doing a database schema change.'"  
    - page: "14"
      content: "This is often referred to as an information radiator, defined by the Agile Alliance as 'the generic term for any of a number of handwritten, drawn, printed, or electronic displays which a team places in a highly visible location, so that all team members as well as passers-by can see the latest information at a glance: count of automated tests, velocity, incident reports, continuous integration status, and so on. This idea originated as part of the Toyota Production System.'"  
    - page: "14"
      content: "'The effectiveness of our monitoring system was highlighted in an instant where our InGraphs monitoring functionality tied to a major web-mail provider started trending downwards and the provider realized they had a problem in their system only after we reached out to them!'"  
    - page: "14"
      content: "Furthermore, after every production incident, we should identify any missing telemetry that could have enabled faster detection and recovery;"  
    - page: "14"
      content: "We want to make as much infrastructure telemetry visible as possible, across all the technology stakeholders, ideally organized by service or application. In other words, when something goes wrong with something in our environment, we need to know exactly what applications and services could be or are being affected.[††] In decades past, creating links between a service and the production infrastructure it depended on was often a manual effort (such as ITIL CMDBs or creating configuration definitions inside alerting tools in tools such as Nagios). However, increasingly these links are now registered automatically within our services, which are then dynamically discovered and used in production through tools such as Zookeeper, Etcd, Consul, etc."  
    - page: "14"
      content: "Regardless of how simple or complex our services are, graphing our business metrics alongside our application and infrastructure metrics allow us to detect when things go wrong. For instance, we may see that new customer signups drop to 20% of daily norms, and then immediately also see that all our database queries are taking five times longer than normal, enabling us to focus our problem solving."  
    - page: "14"
      content: "CONCLUSION The improvements enabled by production telemetry from Etsy and LinkedIn show us how critical it is to see problems as they occur, so we can search out the cause and quickly remedy the situation. By having all elements of our service emitting telemetry that can be analyzed, whether it is in our application, database, or in our environment, and making that telemetry widely available, we can find and fix problems long before they cause something catastrophic, ideally long before a customer even notices that something is wrong. The result is not only happier customers, but, by reducing the amount of firefighting and crises when things go wrong, we have a happier and more productive workplace with less stress and lower levels of burnouts."  
    - page: "14"
      content: "when audits are in progress. By radiating how customers interact with what we build in the context of our goals, we enable fast feedback to feature teams so they can see whether the capabilities we are building are actually being used"  
    - page: "14"
      content: "Figure 28: Amount of user excitement of new features in user forum posts after deployments (Source: Mike Brittain, 'Tracking Every Release,' www.CodeasCraft.com, December 8, 2010,"
    - page: "15"
      content: "Analyze Telemetry to Better Anticipate Problems and Achieve Goals: When people ask me for recommendations on what to monitor, I joke that in an ideal world, we would delete all the alerts we currently have in our monitoring system. Then, after each user-visible outage, we’d ask what indicators would have predicted that outage and then add those to our monitoring system, alerting as needed. Repeat. Now we only have alerts that prevent outages, as opposed to being bombarded by alerts after an outage already occurred."        
    - page: "15"
      content: "In this step, we will replicate the outcomes of such an exercise. One of the easiest ways to do this is to analyze our most severe incidents in the recent past (e.g., 30 days) and create a list of telemetry that could have enabled earlier and faster detection and diagnosis of the problem, as well as easier and faster confirmation that an effective fix had been implemented. For instance, if we had an issue where our NGINX web server stopped responding to requests, we would look at the leading indicators that could have warned us earlier that we were starting to deviate from standard operations, such as: Application level: increasing web page load times, etc. OS level: server free memory running low, disk space running low, etc. Database level: database transaction times taking longer than normal, etc. Network level: number of functioning servers behind the load balancer dropping, etc. Each of these metrics is a potential precursor to a production incident. For each, we would configure our alerting systems to notify them when they deviate sufficiently from the mean, so that we can take corrective action."        
    - page: "15"
      content: "By repeating this process on ever-weaker failure signals, we find problems ever earlier in the life cycle, resulting in fewer customer impacting incidents and near misses. In other words, we are preventing problems as well as enabling quicker detection and correction."        
    - page: "15"
      content: "Many production data sets are non-Gaussian distribution. Dr. Nicole Forsgren explains, In Operations, many of our data sets have what we call ‘chi squared’ distribution. Using standard deviations for this data not only results in over- or under-alerting, but it also results in nonsensical results. She continues, When you compute the number of simultaneous downloads that are three standard deviations below the mean, you end up with a negative number, which obviously doesn’t make sense."        
    - page: "15"
      content: "Case Study—Auto-Scaling Capacity at Netflix (2012) Another tool developed at Netflix to increase service quality, Scryer, addresses some of the shortcomings of Amazon Auto Scaling (AAS), which dynamically increases and decreases AWS compute server counts based on workload data. Scryer works by predicting what customer demands will be based on historical usage patterns and provisions the necessary capacity. Scryer addressed three problems with AAS. The first was dealing with rapid spikes in demand. Because AWS instance startup times can be ten to forty-five minutes, additional compute capacity was often delivered too late to deal with spikes in demand. The second problem was that after outages, the rapid decrease in customer demand led to AAS removing too much compute capacity to handle future incoming demand. The third problem was that AAS didn’t factor in known usage traffic patterns when scheduling compute capacity."        
    - page: "15"
      content: "Figure 32: Netflix customer viewing demand for five days (Source: Daniel Jacobson, Danny Yuan, and Neeraj Joshi, Scryer: Netflix’s Predictive Auto Scaling Engine, The Netflix Tech Blog, November 5, 2013, http://techblog.netflix.com/2013/11/scryer-netflixs- predictive-auto-scaling.html.)"        
    - page: "15"
      content: "USING ANOMALY DETECTION TECHNIQUES When our data does not have Gaussian distribution, we can still find noteworthy variances using a variety of methods. These techniques are broadly categorized as anomaly detection, often defined as the search for items or events which do not conform to an expected pattern. Some of these capabilities can be found inside our monitoring tools, while others may require help from people with statistical skills."        
    - page: "15"
      content: "Case Study—Advanced Anomaly Detection (2014) At Monitorama in 2014, Dr. Toufic Boubez described the power of using anomaly detection techniques, specifically highlighting the effectiveness of the Komogorov-Smirnov test, a technique that is often used in statistics to determine whether two data sets differ significantly and is found in the popular Graphite and Grafana tool. The purpose of presenting this case study here is not as a tutorial, but to demonstrate how a class of statistical techniques can be used in our work, as well as how it’s likely being used in our organizations in completely different applications."        
    - page: "15"
      content: "Figure 35: Transaction volume: under-alerting using 3 standard deviation rule (Source: Dr. Toufic Boubez, Simple math for anomaly detection.) Using the three standard deviation rule would only alert us twice, missing the critical Monday dropoff in transaction volume. Ideally, we would also want to be alerted that the data has drifted from our expected Monday pattern. Even saying ‘Kolmogorov-Smirnov’ is a great way to impress everyone, Dr. Boubez jokes. But what Ops engineers should tell statisticians is that these types of non-parametric techniques are great for Operations data, because it makes no assumptions about normality or any other probability distribution, which is crucial for us to understand what’s going on in our very complex systems. These techniques compare two probability distributions, allowing us to compare periodic or seasonal data, which helps us find variances in data that varies from day to day or week to week."        
    - page: "15"
      content: "Smoothing and other statistical techniques are also used to manipulate graphic and audio files. For instance, image smoothing (or blurring) as each pixel is replaced by the average of all its neighbors. [‡] Other examples of smoothing filters include weighted moving averages or exponential smoothing (which linearly or exponentially weight more recent data points over older data points, respectively), and so forth. [§] Tools we can using to solve these types of problems include Microsoft Excel (which remains one of the easiest and fastest ways to manipulate data for one-time purposes), as well as statistical packages such as SPSS, SAS, and the open source R project, now one of the most widely used statistical packages. Many other tools have been created, including several that Etsy has open-sourced, such as Oculus, which finds graphs with similar shapes that may indicate correlation; Opsweekly, which tracks alert volumes and frequencies; and Skyline, which attempts to identify anomalous behavior in system and application graphs."        
    - page: "15"
      content: "CONCLUSION In this chapter, we explored several different statistical techniques that can be used to analyze our production telemetry so we can find and fix problems earlier than ever, often when they are still small and long before they cause catastrophic outcomes. This enables us to find ever-weaker failure signals that we can then act upon, creating an ever safer system of work, as well as increasing our ability to achieve our goals. Specific case studies were presented, including how Netflix used these techniques to proactively remove compute servers from production and auto-scale their compute infrastructure. We also discussed how to use a moving average and the Kolmogorov-Smirnov filter, both of which can be found in popular telemetry graphing tools. In the next chapter, we will describe how to integrate production telemetry into the daily work of Development in order to make deployments safer and improve the system as a whole."        
    - page: "16"
      content: "Enable Feedback So Development and Operations Can Safely Deploy Code: We start with no one in Dev or Ops being willing to push the deploy code button that we’ve built that automates the entire code deployment process, because of the paralyzing fear of being the first person to potentially bring all of the production systems down. Eventually, when someone is brave enough to volunteer to push their code into production, inevitably, due to incorrect assumptions or production subtleties that weren’t fully appreciated, the first production deployment doesn’t go smoothly—and because we don’t have enough production telemetry, we only find out about the problems when customers tell us. To fix the problem, our team urgently fixes the code and pushes it into production, but this time with more production telemetry added to our applications and environment. This way, we can actually confirm that our fix restored service correctly, and we’ll be able to detect this type of problem before a customer tells us next time. Later, more developers start to push their own code into production. And because we’re working in a complex system, we’ll still probably break something in production, but this time we’ll be able to quickly see what functionality broke, and quickly decide whether to roll back or fix-forward, resolving the problem. This is a huge victory for the entire team and everyone celebrates—we’re now on a roll."        
    - page: "16"
      content: "The Right Media story shows that it is not enough to merely automate the deployment process—we must also integrate the monitoring of production telemetry into our deployment work, as well as establish the cultural norms that everyone is equally responsible for the health of the entire value stream."        
    - page: "16"
      content: "We may choose to turn off broken features with feature toggles (which is often the easiest and least risky option since it involves no deployments to production), or fix forward (i.e., make code changes to fix the defect, which are then pushed into production through the deployment pipeline), or roll back (e.g., switch back to the previous release by using feature toggles or by taking broken servers out of rotation using the blue-green or canary release patterns, etc.)"        
    - page: "16"
      content: "One side effect of this practice is that it helps Development management see that business goals are not achieved simply because features have been marked as done. Instead, the feature is only done when it is performing as designed in production, without causing excessive escalations or unplanned work for either Development or Operations."        
    - page: "16"
      content: "ITIL defines warranty as when a service can run in production reliably without intervention for a predefined period of time (e.g., two weeks). This definition of warranty should ideally be integrated into our collective definition of done."        
    - page: "16"
      content: "HAVE DEVELOPERS FOLLOW WORK DOWNSTREAM One of the most powerful techniques in interaction and user experience design (UX) is contextual inquiry. This is when the product team watches a customer use the application in their natural environment, often working at their desk. Doing so often uncovers startling ways that customers struggle with the application, such as requiring scores of clicks to perform simple tasks in their daily work, cutting and pasting text from multiple screens, or writing down notes on paper. All of these are examples of compensatory behaviors and workarounds for usability issues. The most common reaction for developers after participating in a customer observation is dismay, often stating how awful it was seeing the many ways we have been inflicting pain on our customers. These customer observations almost always result in significant learning and a fervent desire to improve the situation for the customer."        
    - page: "16"
      content: "UX observation often has a powerful impact on the observers. When describing his first customer observation, Gene Kim, the founder and CTO at Tripwire for thirteen years and co-author of this book, said: One of the worst moments of my professional career was in 2006 when I spent an entire morning watching one of our customers use our product. I was watching him perform an operation that we expected customers to do weekly, and, to our extreme horror, we discovered that it required sixty-three clicks. This person kept apologizing, saying things like, Sorry, there’s probably a better way to do this. Unfortunately, there wasn’t a better"        
    - page: "16"
      content: "UX observation enables the creation of quality at the source and results in far greater empathy for fellow team members in the value stream. Ideally, UX observation helps us as we create codified non-functional requirements to add to our shared backlog of work, eventually allowing us to proactively integrate them into every service we build, which is an important part of creating a DevOps work culture."        
    - page: "16"
      content: "By following work downstream, we may uncover ways to help improve flow, such as automating complex, manual steps (e.g., pairing application server clusters that require six hours to successfully complete); performing packaging of code once instead of creating it multiple times at different stages of QA and Production deployment; working with testers to automate manual test suites, thus removing a common bottleneck for more frequent deployment; and creating more useful documentation instead of having someone decipher developer application notes to build packaged"        
    - page: "16"
      content: "following work downstream, we may uncover ways to help improve flow, such as automating complex, manual steps (e.g., pairing application server clusters that require six hours to successfully complete); performing packaging of code once instead of creating it multiple times at different stages of QA and Production deployment; working with testers to automate manual test suites, thus removing a common bottleneck for more frequent deployment; and creating more useful documentation instead of having someone decipher developer application notes to build packaged installers."        
    - page: "16"
      content: "By creating launch guidance, we help ensure that every product team benefits from the cumulative and collective experience of the entire organization, especially Operations. Launch guidance and requirements will likely include the following: Defect counts and severity: Does the application actually perform as designed? Type/frequency of pager alerts: Is the application generating an unsupportable number of alerts in production? Monitoring coverage: Is the coverage of monitoring sufficient to restore service when things go wrong? System architecture: Is the service loosely-coupled enough to support a high rate of changes and deployments in production? Deployment process: Is there a predictable, deterministic, and sufficiently automated process to deploy code into production? Production hygiene: Is there evidence of enough good production habits that would allow production support to be managed by anyone else? Superficially, these requirements may appear similar to traditional production checklists we have used in the past. However, the key differences are we require effective monitoring to be in place, deployments to be reliable and deterministic, and an architecture that supports fast and frequent deployments. If any deficiencies are found during the review, the assigned Ops engineer should help the feature team resolve the issues or even help re-engineer the service if necessary, so that it can be easily deployed and managed in production. At this time, we may also want to learn whether this service is subject to any regulatory compliance objectives or if it is likely to be in the future: Does the service generate a significant amount of revenue? (For example, if it is more than 5% of total revenue of a publicly-held US corporation, it is a significant account and in-scope for compliance with Section 404 of the Sarbanes-Oxley Act of 2002 [SOX].) Does the service have high user traffic or have high outage/impairment costs? (i.e., do operational issues risk creating availability or reputational risk?) Does the service store payment cardholder information, such as credit card numbers, or personally identifiable information, such as Social Security numbers or patient care records? Are there other security issues that could create regulatory, contractual obligation, privacy, or reputation risk? Does the service have any other regulatory or contractual compliance requirements associated with it, such as US export regulations, PCI-DSS, HIPAA, and so forth? This information helps ensure that we effectively manage not only the technical risks associated with this service, but also any potential security and compliance risks. It also provides essential input into the design of the production control environment."        
    - page: "16"
      content: "Figure 39: The Launch readiness review and hand-offs readiness review at Google (Source: SRE@Google: Thousands of DevOps Since 2004, YouTube video, 45:57, posted by USENIX, January 12, 2012, https://www.youtube.com/watch?v=iIuTnhdTzK0.)"        
    - page: "16"
      content: "The LRR must be performed and signed off on before any new Google service is made publicly available to customers and receives live production traffic, while the HRR is performed when the service is transitioned to an Ops-managed state, usually months after the LRR. The LRR and HRR checklists are similar, but the HRR is far more stringent and has higher acceptance standards, while the LRR is self-reported by the product teams."        
    - page: "16"
      content: "Limoncelli noted, In the best case, product teams have been using the LRR checklist as a guideline, working on fulfilling it in parallel with developing their service, and reaching out to SREs to get help when they need it."        
    - page: "16"
      content: "Furthermore, Limoncelli observed, The teams that have the fastest HRR production approval are the ones that worked with SREs earliest, from the early design stages up until launch. And the great thing is, it’s always easy to get an SRE to volunteer to help with your project. Every SRE sees value in giving advice to project teams early, and will likely volunteer a few hours or days to do just that."        
    - page: "16"
      content: "The practice of SREs helping product teams early is an important cultural norm that is continually reinforced at Google. Limoncelli explained, Helping product teams is a long-term investment that will pay off many months later when it comes time to launch. It is a form of ‘good citizenship’ and ‘community service’ that is valued, it is routinely considered when evaluating engineers for SRE promotions."        
    - page: "16"
      content: "In this book, we use the term Ops engineer, but the two terms, Ops Engineer and Site Reliability Engineer, are intended to be interchangeable."        
    - page: "16"
      content: "CONCLUSION In this chapter, we discussed the feedback mechanisms that enable us to improve our service at every stage of our daily work, whether it is deploying changes into production, fixing code when things go wrong and engineers are paged, having developers follow their work downstream, creating non-functional requirements that help development teams write more production-ready code, or even handing problematic services back to be self-managed by Development. By creating these feedback loops, we make production deployments safer, increase the production readiness of code created by Development, and help create a better working relationship between Development and Operations by reinforcing shared goals, responsibilities, and empathy. In the next chapter, we explore how telemetry can enable hypothesis-driven development and A/B testing to perform experiments that help us achieve our organizational goals and win in the marketplace."        
    - page: "17"
      content: "Integrate Hypothesis-Driven Development and A/B Testing into Our Daily Work: In general, Jez Humble observes, the most inefficient way to test a business model or product idea is to build the complete product to see whether the predicted demand actually exists. Before we build a feature, we should rigorously ask ourselves, Should we build it, and why? We should then perform the cheapest and fastest experiments possible to validate through user research whether the intended feature will actually achieve the desired outcomes. We can use techniques such as hypothesis-driven development, customer acquisition funnels, and A/B testing, concepts we explore throughout this chapter."        
    - page: "17"
      content: "Intuit, Inc. provides a dramatic example of how organizations use these techniques to create products that customers love, to promote organizational learning, and to win in the marketplace. Intuit is focused on creating business and financial management solutions to simplify life for small businesses, consumers, and accounting professionals. In 2012, they had $4.5 billion in revenue and 8,500 employees, with flagship products that include QuickBooks, TurboTax, Mint, and, until recently, Quicken."        
    - page: "17"
      content: "Scott Cook, the founder of Intuit, has long advocated building a culture of innovation, encouraging teams to take an experimental approach to product development and exhorting leadership to support them. As he said, Instead of focusing on the boss’s vote…the emphasis is on getting real people to really behave in real experiments, and basing your decisions on that. This is the epitome of a scientific approach to product development."        
    - page: "17"
      content: "Cook explained that what is needed is a system where every employee can do rapid, high-velocity experiments….Dan Maurer runs our consumer division....[which] runs the TurboTax website. When he took over, we did about seven experiments a year. He continued, By installing a rampant innovation culture [in 2010], they now do 165 experiments in the three months of the [US] tax season. Business result? [The] conversion rate of the website is up 50 percent…. The folks [team members] just love it, because now their ideas can make it to market."        
    - page: "17"
      content: "A/B testing techniques were pioneered in direct response marketing, which is one of the two major categories of marketing strategies. The other is called mass marketing or brand marketing and often relies on placing as many ad impressions in front of people as possible to influence buying decisions."        
    - page: "17"
      content: "In previous eras, before email and social media, direct response marketing meant sending thousands of postcards or flyers via postal mail, and asking prospects to accept an offer by calling a telephone number, returning a postcard, or placing an order. In these campaigns, experiments were performed to determine which offer had the highest conversion rates. They experimented with modifying and adapting the offer, re-wording the offer, varying the copywriting styles, design and typography, packaging, and so forth, to determine which was most effective in generating the desired action (e.g., calling a phone number, ordering a product)."        
    - page: "17"
      content: "Each experiment often required doing another design and print run, mailing out thousands of offers, and waiting weeks for responses to come back. Each experiment typically cost tens of thousands of dollars per trial and required weeks or months to complete. However, despite the expense, iterative testing easily paid off if it significantly increased conversion rates (e.g., the percentage of respondents ordering a product going from 3%–12%)."        
    - page: "17"
      content: "There are many other ways to conduct user research before embarking on development. Among the most inexpensive methods include performing surveys, creating prototypes (either mock-ups using tools such as Balsamiq or interactive versions written in code), and performing usability testing. Alberto Savoia, Director of Engineering at Google, coined the term pretotyping for the practice of using prototypes to validate whether we are building the right thing. User research is so inexpensive and easy relative to the effort and cost of building a useless feature in code that, in almost every case, we shouldn’t prioritize a feature without some form of validation."        
    - page: "17"
      content: "INTEGRATING A/B TESTING INTO OUR FEATURE TESTING The most commonly used A/B technique in modern UX practice involves a website where visitors are randomly selected to be shown one of two versions of a page, either a control (the A) or a treatment (the B). Based on statistical analysis of the subsequent behavior of these two cohorts of users, we demonstrate whether there is a significant difference in the outcomes of the two, establishing a causal link between the treatment (e.g., a change in a feature, design element, background color) and the outcome (e.g., conversion rate, average order size)."        
    - page: "17"
      content: "For example, we may conduct an experiment to see whether modifying the text or color on a buy button increases revenue or whether slowing down the response time of a website (by introducing an artificial delay as the treatment) reduces revenue. This type of A/B testing allows us to establish a dollar value on performance improvements."        
    - page: "17"
      content: "Sometimes, A/B tests are also known as online controlled experiments and split tests. It’s also possible to run experiments with more than one variable. This allows us to see how the variables interact, a technique known as multivariate testing."        
    - page: "17"
      content: "The outcomes of A/B tests are often startling. Ronny Kohavi, Distinguished Engineer and General Manager of the Analysis and Experimentation group at Microsoft, observed that after evaluating well-designed and executed experiments that were designed to improve a key metric, only about one-third were successful at improving the key metric! In other words, two-thirds of features either have a negligible impact or actually make things worse. Kohavi goes on to note that all these features were originally thought to be reasonable, good ideas, further elevating the need for user testing over intuition and expert opinions."        
    - page: "17"
      content: "The implications of the Kohavi data are staggering. If we are not performing user research, the odds are that two-thirds of the features we are building deliver zero or negative value to our organization, even as they make our codebase ever more complex, thus increasing our maintenance costs over time and making our software more difficult to change. Furthermore, the effort to build these features is often made at the expense of delivering features that would deliver value (i.e., opportunity cost). Jez Humble joked, Taken to an extreme, the organization and customers would have been better off giving the entire team a vacation, instead of building one of these non–value-adding features."        
    - page: "17"
      content: "Our countermeasure is to integrate A/B testing into the way we design, implement, test, and deploy our features. Performing meaningful user research and experiments ensures that our efforts help achieve our customer and organizational goals, and help us win in the marketplace."        
    - page: "17"
      content: "INTEGRATE A/B TESTING INTO OUR RELEASE Fast and iterative A/B testing is made possible by being able to quickly and easily do production deployments on demand, using feature toggles and potentially delivering multiple versions of our code simultaneously to customer segments. Doing this requires useful production telemetry at all levels of the application stack."        
    - page: "17"
      content: "By hooking into our feature toggles, we can control which percentage of users see the treatment version of an experiment. For example, we may have one-half of our customers be our treatment group and one-half get shown the following: Similar items link on unavailable items in the cart. As part of our experiment, we compare the behavior of the control group (no offer made) against the treatment group (offer made), perhaps measuring number of purchases made in that session."        
    - page: "17"
      content: "Etsy open-sourced their experimentation framework Feature API (formerly known as the Etsy A/B API), which not only supports A/B testing but also online ramp-ups, enabling throttling exposure to experiments. Other A/B testing products include Optimizely, Google Analytics, etc. In a 2014 interview with Kendrick Wang of Apptimize, Lacy Rhoades at Etsy described their journey: Experimentation at Etsy comes from a desire to make informed decisions, and ensure that when we launch features for our millions of members, they work. Too often, we had features that took a lot of time and had to be maintained without any proof of their success or any popularity among users. A/B testing allows us to...say a feature is worth working on as soon as it’s underway."        
    - page: "17"
      content: "INTEGRATING A/B TESTING INTO OUR FEATURE PLANNING Once we have the infrastructure to support A/B feature release and testing, we must ensure that product owners think about each feature as a hypothesis and use our production releases as experiments with real users to prove or disprove that hypothesis. Constructing experiments should be designed in the context of the overall customer acquisition funnel."        
    - page: "17"
      content: "Barry O’Reilly, co-author of Lean Enterprise: How High Performance Organizations Innovate at Scale, described how we can frame hypotheses in feature development in the following form: We Believe that increasing the size of hotel images on the booking page Will Result in improved customer engagement and conversion We Will Have Confidence To Proceed When we see a 5% increase in customers who review hotel images who then proceed to book in forty-eight hours. Adopting an experimental approach to product development requires us to not only break down work into small units (stories or requirements), but also validate whether each unit of work is delivering the expected outcomes. If it does not, we modify our road map of work with alternative paths that will actually achieve those outcomes."        
    - page: "17"
      content: "As the Yahoo! Answers team was able to move to weekly deployments, and later multiple deployments per week, their ability to experiment with new features increased dramatically. Their astounding achievements and learnings over the next twelve months of experimentation included increased monthly visits of 72%, increased user engagement of threefold, and the team doubled their revenue. To continue their success, the team focused on optimizing the following top metrics: Time to first answer: How quickly was an answer posted to a user question? Time to best answer: How quickly did the user community award a best answer? Upvotes per answer: How many times was an answer upvoted by the user community? Answers/week/person: How many answers were users creating? Second search rate: How often did visitors have to search again to get an answer? (Lower is better.) Stoneham concluded, This was exactly the learning that we needed to win in the marketplace—and it changed more than our feature velocity. We transformed from a team of employees to a team of owners. When you move at that speed, and are looking at the numbers and the results daily, your investment level radically changes."        
    - page: "17"
      content: "CONCLUSION Success requires us to not only deploy and release software quickly, but also to out-experiment our competition. Techniques such as hypothesis-driven development, defining and measuring out customer acquisition funnel, and A/B testing allow us to perform user-experiments safely and easily, enabling us to unleash creativity and innovation, and create organizational learning. And, while succeeding is important, the organizational learning that comes from experimentation also gives employees ownership of business objectives and customer satisfaction. In the next chapter, we examine and create review and coordination processes as a way to increase the quality of our current work."        
    - page: "18"
      content: "Create Review and Coordination Processes to Increase Quality of Our Current Work: Create Review and Coordination Processes to Increase Quality of Our Current Work In the previous chapters, we created the telemetry necessary to see and solve problems in production and at all stages of our deployment pipeline, and created fast feedback loops from customers to help enhance organizational learning—learning that encourages ownership and responsibility for customer satisfaction and feature performance, which helps us succeed. Our goal in this chapter is to enable Development and Operations to reduce the risk of production changes before they are made."        
    - page: "18"
      content: "The peer review process at GitHub is a striking example of how inspection can increase quality, make deployments safe, and be integrated into the flow of everyone’s daily work. They pioneered the process called pull request, one of the most popular forms of peer review that span Dev and Ops."        
    - page: "18"
      content: "Scott Chacon, CIO and co-founder of GitHub, wrote on his website that pull requests are the mechanism that lets engineers tell others about changes they have pushed to a repository on GitHub. Once a pull request is sent, interested parties can review the set of changes, discuss potential modifications, and even push follow-up commits if necessary. Engineers submitting a pull request will often request a +1, +2, or so forth, depending on how many reviews they need, or @mention engineers that they’d like to get reviews from. At GitHub, pull requests are also the mechanism used to deploy code into production through a collective set of practices they call GitHub Flow—it’s how engineers request code reviews, gather and integrate feedback, and announce that code will be deployed to production (i.e., master branch)."        
    - page: "18"
      content: "GitHub Flow is composed of five steps: To work on something new, the engineer creates a descriptively named branch off of master (e.g., new-oauth2-scopes). The engineer commits to that branch locally, regularly pushing their work to the same named branch on the server. When they need feedback or help, or when they think the branch is ready for merging, they open a pull request. When they get their desired reviews and get any necessary approvals of the feature, the engineer can then merge it into master. Once the code changes are merged and pushed to master, the engineer deploys them into production. These practices, which integrate review and coordination into daliy work, have allowed GitHub to quickly and reliably deliver features to market with high quality and security. For example, in 2012 they performed an amazing 12,602 deployments. In particular, on August 23rd, after a company-wide summit where many exciting ideas were brainstormed and discussed, the company had their busiest deployment day of the year, with 563 builds and 175 successful deployments into production, all made possible through the pull request process."        
    - page: "18"
      content: "THE DANGERS OF CHANGE APPROVAL PROCESSES The Knight Capital failure is one of the most prominent software deployment errors in recent memory. A fifteen minute deployment error resulted in a $440 million trading loss, during which the engineering teams were unable to disable the production services. The financial losses jeopardized the firm’s operations and forced the company to be sold over the weekend so they could continue operating without jeopardizing the entire financial system. John Allspaw observed that when high-profile incidents occur, such as the Knight Capital deployment accident, there are typically two counterfactual narratives for why the accident occurred."        
    - page: "18"
      content: "The first narrative is that the accident was due to a change control failure, which seems valid because we can imagine a situation where better change control practices could have detected the risk earlier and prevented the change from going into production. And if we couldn’t prevent it, we might have taken steps to enable faster detection and recovery. The second narrative is that the accident was due to a testing failure. This also seems valid, with better testing practices we could have identified the risk earlier and canceled the risky deployment, or we could have at least taken steps to enable faster detection and recovery. The surprising reality is that in environments with low-trust, command-and-control cultures, the outcomes of these types of change control and testing countermeasures often result in an increased likelihood that problems will occur again, potentially with even worse outcomes. Gene Kim (co-author of this book) describes his realization that change and testing controls can potentially have the opposite effect than intended as one of the most important moments of my professional career. This ‘aha’ moment was the result of a conversation in 2013 with John Allspaw and Jez Humble about the Knight Capital accident, making me question some of my core beliefs that I’ve formed over the last ten years, especially having been trained as an auditor. He continues, However upsetting it was, it was also a very formative moment for me. Not only did they convince me that they were correct, we tested these beliefs in the 2014 State of DevOps Report, which led to some astonishing findings that reinforce that building high-trust cultures is likely the largest management challenge of this decade."        
    - page: "18"
      content: "Counterfactual thinking is a term used in psychology that involves the human tendency to create possible alternatives to life events that have already occurred. In reliability engineering, it often involves narratives of the system as imagined as opposed to the system in reality."        
    - page: "18"
      content: "One of the core beliefs in the Toyota Production System is that people closest to a problem typically know the most about it. This becomes more pronounced as the work being performed and the system the work occurs in become more complex and dynamic, as is typical in DevOps value streams. In these cases, creating approval steps from people who are located further and further away from the work may actually reduce the likelihood of success. As has been proven time and again, the further the distance between the person doing the work (i.e., the change implementer) and the person deciding to do the work (i.e., the change authorizer), the worse the outcome."        
    - page: "18"
      content: "In Puppet Labs’ 2014 State of DevOps Report, one of the key findings was that high-performing organizations relied more on peer review and less on external approval of changes. Figure 41 shows that the more organizations rely on change approvals, the worse their IT performance in terms of both stability (mean time to restore service and change fail rate) and throughput (deployment lead times, deployment frequency)."        
    - page: "18"
      content: "For all these reasons, we need to create effective control practices that more closely resemble peer review, reducing our reliance on external bodies to authorize our changes. We also need to coordinate and schedule changes effectively. We explore both of these in the next two sections."        
    - page: "18"
      content: "ENABLE PEER REVIEW OF CHANGES Instead of requiring approval from an external body prior to deployment, we may require engineers to get peer reviews of their changes. In Development, this practice has been called code review, but it is equally applicable to any change we make to our applications or environments, including servers, networking, and databases. The goal is to find errors by having fellow engineers close to the work scrutinize our changes. This review improves the quality of our changes, which also creates the benefits of cross-training, peer learning, and skill improvement. A logical place to require reviews is prior to committing code to trunk in source control, where changes could potentially have a team-wide or global impact. At a minimum, fellow engineers should review our change, but for higher risk areas, such as database changes or business-critical components with poor automated test coverage, we may require further review from a subject matter expert (e.g., information security engineer, database engineer) or multiple reviews (e.g., +2 instead of merely +1)."        
    - page: "18"
      content: "The principle of small batch sizes also applies to code reviews. The larger the size of the change that needs to be reviewed, the longer it takes to understand and the larger the burden on the reviewing engineer. As Randy Shoup observed, There is a non-linear relationship between the size of the change and the potential risk of integrating that change—when you go from a ten line code change to a one hundred line code, the risk of something going wrong is more than ten times higher, and so forth. This is why it’s so essential for developers to work in small, incremental steps rather than on long-lived feature branches."        
    - page: "18"
      content: "Furthermore, our ability to meaningfully critique code changes goes down as the change size goes up. As Giray Özil tweeted, Ask a programmer to review ten lines of code, he’ll find ten issues. Ask him to do five hundred lines, and he’ll say it looks good. Guidelines for code reviews include: Everyone must have someone to review their changes (e.g., to the code, environment, etc.) before committing to trunk. Everyone should monitor the commit stream of their fellow team members so that potential conflicts can be identified and reviewed. Define which changes qualify as high risk and may require review from a designated subject matter expert (e.g., database changes, security-sensitive modules such as authentication, etc.).[§] If someone submits a change that is too large to reason about easily—in other words, you can’t understand its impact after reading through it a couple of times, or you need to ask the submitter for clarification—it should be split up into multiple, smaller changes that can be understood at a glance."        
    - page: "18"
      content: "Code reviews come in various forms: Pair programming: programmers work in pairs (see section below) Over-the-shoulder: One developer looks over the author’s shoulder as the latter walks through the code. Email pass-around: A source code management system emails code to reviewers automatically after the code is checked in. Tool-assisted code review: Authors and reviewers use specialized tools designed for peer code review (e.g., Gerrit, GitHub pull requests, etc.) or facilities provided by the source code repositories (e.g., GitHub, Mercurial, Subversion, as well as other platforms such as Gerrit, Atlassian Stash, and Atlassian Crucible). Close scrutiny of changes in many forms is effective in locating errors previously overlooked."        
    - page: "18"
      content: "Case Study—Code Reviews at Google (2010) Google is an excellent example of a company that employees trunk-based development and continuous delivery at scale. As noted earlier in this book, Eran Messeri described that in 2013 the processes at Google enabled over thirteen thousand developers to work off of trunk on a single source code tree, performing over 5,500 code commits per week, resulting in hundreds of production deployments per week. In 2010, there were 20+ changes being checked in to trunk every minute, resulting in 50% of the codebase being changed every month. This requires considerable discipline from Google team members and mandatory code reviews, which cover the following areas: Code readability for languages (enforces style guide) Ownership assignments for code sub-trees to maintain consistency and correctness Code transparency and code contributions across teams"        
    - page: "18"
      content: "Figure 42 shows how code review lead times are affected by the change size. On the x-axis is the size of the change, and on the y-axis is the lead time required for code review process. In general, the larger the change submitted for code reviews, the longer the lead time required to get the necessary sign offs. And the data points in the upper-left corner represent the more complex and potentially risky changes that required more deliberation and discussion. Figure 42: Size of change vs. lead time for reviews at Google (Source: Ashish Kumar, Development at the Speed and Scale of Google, presentation at QCon, San Francisco, CA, 2010, https://qconsf.com/sf2010/dl/qcon-sanfran-2010/slides/ AshishKumar_DevelopingProductsattheSpeedandScaleofGoogle.pdf.)"        
    - page: "18"
      content: "POTENTIAL DANGERS OF DOING MORE MANUAL TESTING AND CHANGE FREEZES Now that we have created peer reviews that reduce our risk, shorten lead times associated with change approval processes, and enable continuous delivery at scale, such as we saw in the Google case study, let us examine the effects of how testing countermeasure can sometimes backfire. When testing failures occur, our typical reaction is to do more testing. However, if we are merely performing more testing at the end of the project, we may worsen our outcomes. This is especially true if we are doing manual testing, because manual testing is naturally slower and more tedious than automated testing and performing additional testing often has the consequence of taking significantly longer to test, which means we are deploying less frequently, thus increasing our deployment batch size. And we know from both theory and practice that when we increase our deployment batch size, our change success rates go down and our incident counts and MTTR go up—the opposite of the outcome we want. Instead of performing testing on large batches of changes that are scheduled around change freeze periods, we want to fully integrate testing our daily work as part of the smooth and continual flow into production, and increase our deployment frequency. By doing this, we build in quality, which allows us to test, deploy, and release in ever smaller batch sizes."        
    - page: "18"
      content: "ENABLE PAIR PROGRAMMING TO IMPROVE ALL OUR CHANGES Pair programming is when two engineers work together at the same workstation, a method popularized by Extreme Programming and Agile in the early 2000s. As with code reviews, this practice started in Development but is equally applicable to the work that any engineer does in our value stream. In this book, we will use the term pairing and pair programming interchangeably, to indicate that the practice is not just for developers. In one common pattern of pairing, one engineer fills the role of the driver, the person who actually writes the code, while the other engineer acts as the navigator, observer, or pointer, the person who reviews the work as it is being performed. While reviewing, the observer may also consider the strategic direction of the work, coming up with ideas for improvements and likely future problems to address. This frees the driver to focus all of his or her attention on the tactical aspects of completing the task, using the observer as a safety net and guide. When the two have differing specialties, skills are transferred as an automatic side effect, whether it’s through ad-hoc training or by sharing techniques and workarounds."        
    - page: "18"
      content: "Another pair programming pattern reinforces test-driven development (TDD) by having one engineer write the automated test and the other engineer implement the code. Jeff Atwood, one of the founders of Stack Exchange, wrote, I can’t help wondering if pair programming is nothing more than code review on steroids….The advantage of pair programming is its gripping immediacy: it is impossible to ignore the reviewer when he or she is sitting right next to you. He continued, Most people will passively opt out [of reviewing code] if given the choice. With pair programming, thats not possible. Each half of the pair has to understand the code, right then and there, as its being written. Pairing may be invasive, but it can also force a level of communication that youd otherwise never achieve."        
    - page: "18"
      content: "Dr. Laurie Williams performed a study in 2001 that showed paired programmers are 15% slower than two independent individual programmers, while ‘error-free code increased from 70% to 85%. Since testing and debugging are often many times more costly than initial programming, this is an impressive result. Pairs typically consider more design alternatives than programmers working alone and arrive at simpler, more maintainable designs; they also catch design defects early. Dr. Williams also reported that 96% of her respondents stated that they enjoyed their work more when they programmed in pairs than when they programmed alone.[¶] Pair programming has the additional benefit of spreading knowledge throughout the organization and increasing information flow within the team. Having more experienced engineers review while the less experienced engineer codes is also an effective way to teach and be taught."        
    - page: "18"
      content: "Another method comes from Ryan Tomayko, CIO and co-founder of GitHub and one of the inventors of the pull request process. When asked to describe the difference between a bad pull request and a good pull request, he said it has little to do with the production outcome. Instead, a bad pull request is one that doesnt have enough context for the reader, having little or no documentation of what the change is intended to do. For example, a pull request that merely has the following text: Fixing issue #3616 and #3841.[**] That was an actual internal GitHub pull request, which Tomayko critiqued, This was probably written by a new engineer here. First off, no specific engineers were specifically @mentioned—at a minimum, the engineer should have mentioned their mentor or a subject matter expert in the area that theyre modifying to ensure that someone appropriate reviews their change. Worse, there isnt any explanation of what the changes actually are, why its important, or exposing any of the implementers thinking. On the other hand, when asked to describe a great pull request that indicates an effective review process, Tomayko quickly listed off the essential elements: there must be sufficient detail on why the change is being made, how the change was made, as well as any identified risks and resulting countermeasures. Tomayko also looks for good discussion of the change, enabled by all the context that the pull request provided—often, there will be additional risks pointed out, ideas on better ways to implement the desired change, ideas on how to better mitigate the risk, and so forth. And if something bad or unexpected happens upon deployment, it is added to the pull request, with a link to the corresponding issue. All discussion happens without placing blame; instead, there is a candid conversation on how to prevent the problem from recurring in the future. As an example, Tomayko produced another internal GitHub pull request for a database migration. It was many pages long, with lengthy discussions about the potential risks, leading up to the following statement by the pull request author: I am pushing this now. Builds are now failing for the branch, because of a missing column in the CI servers. (Link to Post-Mortem: MySQL outage) The change submitter then apologized for the outage, describing what conditions and mistaken assumptions led to the accident, as well as a list of proposed countermeasures to prevent it from happening again. This was followed by pages and pages of discussion. Reading through the pull request, Tomayko smiled, Now that is a great pull request."        
    - page: "18"
      content: "CONCLUSION In this chapter, we discussed how to integrate practices into our daily work that increase the quality of our changes and reduce the risk of poor deployment outcomes, reducing our reliance on approval processes. Case studies from GitHub and Target show that these practices not only improve our outcomes, but also significantly reduce lead times and increase developer productivity. To do this kind of work requires a high-trust culture. Consider a story that John Allspaw told about a newly hired junior engineer: The engineer asked if it was okay to deploy a small HTML change, and Allspaw responded, I dont know, is it? He then asked Did you have someone review your change? Do you know who the best person to ask is for changes of this type? Did you do everything you absolutely could to assure yourself that this change operates in production as designed? If you did, then dont ask me—just make the change! By responding this way, Allspaw reminded the engineer that she was solely responsibility for the quality of her change—if she did everything she felt she could to give herself confidence that the change would work, then she didnt need to ask anyone for approval, she should make the change. Creating the conditions that enable change implementers to fully own the quality of their changes is an essential part of the high-trust, generative culture we are striving to build. Furthermore, these conditions enable us to create an ever-safer system of work, where we are all helping each other achieve our goals, spanning whatever boundaries necessary to get there."        
    - page: "18"
      content: "PART IV CONCLUSION Part IV has shown us that by implementing feedback loops we can enable everyone to work together toward shared goals, see problems as they occur, and, with quick detection and recovery, ensure that features not only operate as designed in production, but also achieve organizational goals and organizational learning. We have also examined how to enable shared goals spanning Dev and Ops so that they can improve the health of the entire value stream. We are now ready to enter Part V: The Third Way, The Technical Practices of Learning, so we can create opportunities for learning that happen earlier and ever more quickly and cheaply, and so that we can unleash a culture of innovation and experimentation that enables everyone to do meaningful work that helps our organization succeed."        
    - page: "19"
      content: "Enable and Inject Learning into Daily Work - Dr. Spear summarizes ONeills accomplishments at Alcoa when he writes, Though it started by focusing on problems related to workplace safety, it soon found that safety problems reflected process ignorance and that this ignorance would also manifest itself in other problems such as quality, timeliness, and yield versus scrap."        
    - page: "19"
      content: "The way NASA handled failure signals during the space shuttle era serves as an illustrative example: In 2003, sixteen days into the Columbia space shuttle mission, it exploded as it re-entered the earths atmosphere. We now know that a piece of insulating foam had broken off the external fuel tank during takeoff. However, prior to Columbias re-entry, a handful of mid-level NASA engineers had reported this incident, but their voices had gone unheard. They observed the foam strike on video monitors during a post-launch review session and immediately notified NASAs managers, but they were told that the foam issue was nothing new. Foam dislodgement had damaged shuttles in previous launches, but had never resulted in an accident. It was considered a maintenance problem and not acted upon until it was too late."        
    - page: "19"
      content: "Michael Roberto, Richard M.J. Bohmer, and Amy C. Edmondson wrote in a 2006 article for Harvard Business Review how NASA culture contributed to this problem. They describe how organizations are typically structured in one of two models: a standardized model, where routine and systems govern everything, including strict compliance with timelines and budgets, or an experimental model, where every day every exercise and every piece of new information is evaluated and debated in a culture that resembles a research and design (R&D) laboratory. They observe, Firms get into trouble when they apply the wrong mind-set to an organization [which dictates how they respond to ambiguous threats or, in the terminology of this book, weak failure signals]....By the 1970s, NASA had created a culture of rigid standardization, promoting to Congress the space shuttle as a cheap and reusable spacecraft. NASA favored strict process compliance instead of an experimental model where every piece of information needed to be evaluated as it occured without bias. The absence of continuous learning and experimentation had dire consequences. The authors conclude that it is culture and mind-set that matters, not just being careful—as they write, vigilance alone will not prevent ambiguous threats [weak failure signals] from turning into costly (and sometimes tragic) failures."      
    - page: "19"
      content: "REDEFINE FAILURE AND ENCOURAGE CALCULATED RISK-TAKING Leaders of an organization, whether deliberately or inadvertently, reinforce the organizational culture and values through their actions. Audit, accounting, and ethics experts have long observed that the tone at the top predicts the likelihood of fraud and other unethical practices. To reinforce our culture of learning and calculated risk-taking, we need leaders to continually reinforce that everyone should feel both comfortable with and responsible for surfacing and learning from failures."
    - page: "19"
      content: "On failures, Roy Rapoport from Netflix observes, What the 2014 State of DevOps Report proved to me is that high-performing DevOps organizations will fail and make mistakes more often. Not only is this okay, its what organizations need! You can even see it in the data: if high performers are performing thirty times more frequently but with only half the change failure rate, theyre obviously having more failures. He continues, I was talking with a co-worker about a massive outage we just had at Netflix—it was caused by, frankly, a dumb mistake. In fact, it was caused by an engineer who had taken down Netflix twice in the last eighteen months. But, of course, this is a person wed never fire. In that same eighteen months, this engineer moved the state of our operations and automation forward not by miles but by light-years. That work has enabled us to do deployments safely on a daily basis, and has personally performed huge numbers of production deployments. He concludes, DevOps must allow this sort of innovation and the resulting risks of people making mistakes. Yes, youll have more failures in production. But thats a good thing, and should not be punished."        
    - page: "19"
      content: "Robbins observes that whenever you set out to engineer a system at scale, the best you can hope for is to build a reliable software platform on top of components that are completely unreliable. That puts you in an environment where complex failures are both inevitable and unpredictable."        
    - page: "19"
      content: "Our goal for Game Day is to help teams simulate and rehearse accidents to give them the ability to practice. First, we schedule a catastrophic event, such as the simulated destruction of an entire data center, to happen at some point in the future. We then give teams time to prepare, to eliminate all the single points of failure, and to create the necessary monitoring procedures, failover procedures, etc. Our Game Day team defines and executes drills, such as conducting database failovers (i.e., simulating a database failure and ensuring that the secondary database works) or turning off an important network connection to expose problems in the defined processes. Any problems or difficulties that are encountered are identified, addressed, and tested again."        
    - page: "19"
      content: "An excellent example of simulating disaster is Googles Disaster Recovery Program (DiRT). Kripa Krishnan is a technical program director at Google, and, at the time of this writing, has led the program for over seven years. During that time, theyve simulated an earthquake in Silicon Valley, which resulted in the entire Mountain View campus being disconnected from Google; major data centers having complete loss of power; and even aliens attacking cities where engineers resided. As Krishnan wrote, An often-overlooked area of testing is business process and communications. Systems and processes are highly intertwined, and separating testing of systems from testing of business processes isnt realistic: a failure of a business system will affect the business process, and conversely a working system is not very useful without the right personnel. Some of the learnings gained during these disasters included: When connectivity was lost, the failover to the engineer workstations didnt work Engineers didnt know how to access a conference call bridge or the bridge only had capacity for fifty people or they needed a new conference call provider who would allow them to kick off engineers who had subjected the entire conference to hold music When the data centers ran out of diesel for the backup generators, no one knew the procedures for making emergency purchases through the supplier, resulting in someone using a personal credit card to purchase $50,000 worth of diesel. By creating failure in a controlled situation, we can practice and create the playbooks we need. One of the other outputs of Game Days is that people actually know who to call and know who to talk to—by doing this, they develop relationships with people in other departments so they can work together during an incident, turning conscious actions into unconscious actions that are able to become routine."        
    - page: "19"
      content: "CONCLUSION To create a just culture that enables organizational learning, we have to re-contextualize so-called failures. When treated properly, errors that are inherent in complex systems can create a dynamic learning environment where all of the shareholders feel safe enough to come forward with ideas and observations, and where groups rebound more readily from projects that dont perform as expected. Both blameless post-mortems and injecting production failures reinforce a culture that everyone should feel both comfortable with and responsible for surfacing and learning from failures. In fact, when we sufficiently reduce the number of accidents, we decrease our tolerance so that we can keep learning. As Peter Senge is known to say, The only sustainable competitive advantage is an organizations ability to learn faster than the competition."  
    - page: "20"
      content: "Convert Local Discoveries into Global Improvements"
    - page: "20"
      content: "By doing this, we elevate the state of the practice of the entire organization so that everyone doing work benefits from the cumulative experience of the organization."
    - page: "20"
      content: "In this chapter, we will create mechanisms that make it possible for new learnings and improvements discovered locally to be captured and shared globally throughout the entire organization, multiplying the effect of global knowledge and improvement. By doing this, we elevate the state of the practice of the entire organization so that everyone doing work benefits from the cumulative experience of the organization."
    - page: "20"
      content: "USE CHAT ROOMS AND CHAT BOTS TO AUTOMATE AND CAPTURE ORGANIZATIONAL KNOWLEDGE"
    - page: "20"
      content: "AUTOMATE STANDARDIZED PROCESSES IN SOFTWARE FOR RE-USE"
    - page: "20"
      content: "AUTOMATE STANDARDIZED PROCESSES IN SOFTWARE FOR RE-USE All too often, we codify our standards and processes for architecture, testing, deployment, and infrastructure management in prose, storing them in Word documents that are uploaded somewhere. The problem is that engineers who are building new applications or environments often dont know that these documents exist, or they dont have the time to implement the documented standards. The result is they create their own tools and processes, with all the disappointing outcomes wed expect: fragile, insecure, and unmaintainable applications and environments that are expensive to run, maintain, and evolve. Instead of putting our expertise into Word documents, we need to transform these documented standards and processes, which encompass the sum of our organizational learnings and knowledge, into an executable form that makes them easier to reuse. One of the best ways we can make this knowledge re-usable is by putting it into a centralized source code repository, making the tool available for everyone to search and use."
    - page: "20"
      content: "Justin Arbuckle was chief architect at GE Capital in 2013 when he said, We needed to create a mechanism that would allow teams to easily comply with policy—national, regional, and industry regulations across dozens of regulatory frameworks, spanning thousands of applications running on tens of thousands of servers in tens of data centers. The mechanism they created was called ArchOps, which enabled our engineers to be builders, not bricklayers. By putting our design standards into automated blueprints that were able to be used easily by anyone, we achieved consistency as a byproduct."
    - page: "20"
      content: "CREATE A SINGLE, SHARED SOURCE CODE REPOSITORY FOR OUR ENTIRE ORGANIZATION A firm-wide, shared source code repository is one of the most powerful mechanisms used to integrate local discoveries across the entire organization. When we update anything in the source code repository (e.g., a shared library), it rapidly and automatically propagates to every other service that uses that library, and it is integrated through each teams deployment pipeline."
    - page: "20"
      content: "Google is one of the largest examples of using an organization-wide shared source code repository. By 2015, Google had a single shared source code repository with over one billion files and over two billion lines of code. This repository is used by every one of their twenty-five thousand engineers and spans every Google property, including Google Search, Google Maps, Google Docs, Google+, Google Calendar, Gmail, and YouTube. One of the valuable results of this is that engineers can leverage the diverse expertise of everyone in the organization. Rachel Potvin, a Google engineering manager overseeing the Developer Infrastructure group, told Wired that every Google engineer can access a wealth of libraries because almost everything has already been done."
    - page: "20"
      content: "Furthermore, as Eran Messeri, an engineer in the Google Developer Infrastructure group, explains, one of the advantages of using a single repository is that it allows users to easily access all of the code in its most up-to-date form, without the need for coordination."
    - page: "20"
      content: "We put into our shared source code repository not only source code, but also other artifacts that encode knowledge and learning, including: Configuration standards for our libraries, infrastructure, and environments (Chef recipes, Puppet manifests, etc.) Deployment tools Testing standards and tools, including security Deployment pipeline tools Monitoring and analysis tools Tutorials and standards"
    - page: "20"
      content: "Tom Limoncelli is the co-author of The Practice of Cloud System Administration: Designing and Operating Large Distributed Systems and a former Site Reliability Engineer at Google. In his book, he states that the value of having a single repository for an entire organization is so powerful it is difficult to even explain. You can write a tool exactly once and have it be usable for all projects. You have 100% accurate knowledge of who depends on a library; therefore, you can refactor it and be 100% sure of who will be affected and who needs to test for breakage. I could probably list one hundred more examples. I cant express in words how much of a competitive advantage this is for Google."
    - page: "20"
      content: "In this model, the library owner is also responsible for safely migrating each group using the repository from one version to the next. This in turn requires quick detection of regression errors through comprehensive automated testing and continuous integration for all systems that use the library."
    - page: "20"
      content: "SPREAD KNOWLEDGE BY USING AUTOMATED TESTS AS DOCUMENTATION AND COMMUNITIES OF PRACTICE"
    - page: "20"
      content: "In order to more rapidly propagate knowledge, we can also create discussion groups or chat rooms for each library or service, so anyone who has questions can get responses from other users, who are often faster to respond than the developers."
    - page: "20"
      content: "DESIGN FOR OPERATIONS THROUGH CODIFIED NON-FUNCTIONAL REQUIREMENTS When Development follows their work downstream and participates in production incident resolution activities, the application becomes increasingly better designed for Operations."
    - page: "20"
      content: "Implementing these non-functional requirements will enable our services to be easy to deploy and keep running in production, where we can quickly detect and correct problems, and ensure it degrades gracefully when components fail. Examples of non-functional requirements include ensuring that we have: Sufficient production telemetry in our applications and environments The ability to accurately track dependencies Services that are resilient and degrade gracefully Forward and backward compatibility between versions The ability to archive data to manage the size of the production data set The ability to easily search and understand log messages across services The ability to trace requests from users through multiple services Simple, centralized runtime configuration using feature flags and so forth"
    - page: "20"
      content: "By codifying these types of non-functional requirements, we make it easier for all our new and existing services to leverage the collective knowledge and experience of the organization. These are all responsibilities of the team building the service."
    - page: "20"
      content: "CONCLUSION The techniques described in this chapter enable every new learning to be incorporated into the collective knowledge of the organization, multiplying its effect. We do this by actively and widely communicating new knowledge, such as through chat rooms and through technology such as architecture as code, shared source code repositories, technology standardization, and so forth. By doing this, we elevate the state of the practice of not just Dev and Ops, but also the entire organization, so everyone who performs work does so with the cumulative experience of the entire organization."
    - page: "21"
      content: "Reserve Time to Create Organizational Learning and Improvement"
    - page: "21"
      content: "INSTITUTIONALIZE RITUALS TO PAY DOWN TECHNICAL DEBT In this section, we schedule rituals that help enforce the practice of reserving Dev and Ops time for improvement work, such as non-functional requirements, automation, etc. One of the easiest ways to do this is to schedule and conduct day- or week-long improvement blitzes, where everyone on a team (or in the entire organization) self-organizes to fix problems they care about—no feature work is allowed."
    - page: "21"
      content: "Our goal during these blitzes is not to simply experiment and innovate for the sake of testing out new technologies, but to improve our daily work, such as solving our daily workarounds. While experiments can also lead to improvements, improvement blitzes are very focused on solving specific problems we encounter in our daily work."
    - page: "21"
      content: "These improvement blitzes are simple to administer: One week is selected where everyone in the technology organization works on an improvement activity at the same time. At the end of the period, each team makes a presentation to their peers that discusses the problem they were tackling and what they built. This practice reinforces a culture in which engineers work across the entire value stream to solve problems. Furthermore, it reinforces fixing problems as part of our daily work and demonstrates that we value paying down technical debt."
    - page: "21"
      content: "What makes improvement blitzes so powerful is that we are empowering those closest to the work to continually identify and solve their own problems."
    - page: "21"
      content: "A great example of the success of the improvement blitz concept is described by Mark Zuckerberg, Facebook CEO. In an interview with Jessica Stillman of www.Inc.com, he says, Every few months we have a hackathon, where everyone builds prototypes for new ideas they have. At the end, the whole team gets together and looks at everything that has been built. Many of our most successful products came out of hackathons, including Timeline, chat, video, our mobile development framework and some of our most important infrastructure like the HipHop compiler."
    - page: "21"
      content: "ENABLE EVERYONE TO TEACH AND LEARN A dynamic culture of learning creates conditions so that everyone can not only learn, but also teach, whether through traditional didactic methods (e.g., people taking classes, attending training) or more experiential or open methods (e.g., conferences, workshops, mentoring). One way that we can foster this teaching and learning is to dedicate organizational time to it."
    - page: "21"
      content: "The most valuable thing any associate can do is mentor or learn from other associates."
    - page: "21"
      content: "As has been made evident throughout this book, certain skills are becoming increasingly needed by all engineers, not just by developers. For instance, it is becoming more important for all Operations and Test engineers to be familiar with Development techniques, rituals, and skills, such as version control, automated testing, deployment pipelines, configuration management, and creating automation. Familiarity with Development techniques helps Operations engineers remain relevant as more technology value streams adopt DevOps principles and patterns."
    - page: "21"
      content: "Although the prospect of learning something new may be intimidating or cause a sense of embarrassment or shame, it shouldnt. After all, we are all lifelong learners, and one of the best ways to learn is from our peers. Karthik Gaekwad, who was part of the National Instruments DevOps transformation, said, For Operations people who are trying to learn automation, it shouldnt be scary—just ask a friendly developer, because they would love to help."
    - page: "21"
      content: "For all technology professionals who love innovating, love change, there is a wonderful and vibrant future ahead of us."
    - page: "21"
      content: "As Glenn ODonnell from Forrester Research quipped in his 2014 DevOps Enterprise Summit presentation, For all technology professionals who love innovating, love change, there is a wonderful and vibrant future ahead of us."
    - page: "21"
      content: "SHARE YOUR EXPERIENCES FROM DEVOPS CONFERENCES In many cost-focused organizations, engineers are often discouraged from attending conferences and learning from their peers. To help build a learning organization, we should encourage our engineers (both from Development and Operations) to attend conferences, give talks at them, and, when necessary, create and organize internal or external conferences themselves."
    - page: "21"
      content: "DevOpsDays remains one of the most vibrant self-organized conference series today. Many DevOps practices have been shared and promulgated at these events. It has remained free or nearly free, supported by a vibrant community of practitioner communities and vendors."
    - page: "21"
      content: "CREATE INTERNAL CONSULTING AND COACHES TO SPREAD PRACTICES Creating an internal coaching and consulting organization is a method commonly used to spread expertise across an organization. This can come in many different forms. At Capital One, designated subject matter experts hold office hours where anyone can consult with them, ask questions, etc."
    - page: "21"
      content: "20% innovation time policy at Google, enabling developers to spend roughly one day per week on a Google-related project outside of their primary area of responsibility. Some engineers chose to form grouplets, ad hoc teams of like-minded engineers who wanted to pool their 20% time, allowing them to do focused improvement blitzes. A testing grouplet was formed by Bharat Mediratta and Nick Lesiecki, with the mission of driving the adoption of automated testing across Google. Even though they had no budget or formal authority, as Mike Bland described, There were no explicit constraints put upon us, either. And we took advantage of that. They used several mechanisms to drive adoption, but one of the most famous was Testing on the Toilet (or TotT), their weekly testing periodical. Each week, they published a newsletter in nearly every bathroom in nearly every Google office worldwide."
    - page: "21"
      content: "A testing grouplet was formed by Bharat Mediratta and Nick Lesiecki, with the mission of driving the adoption of automated testing across Google. Even though they had no budget or formal authority, as Mike Bland described, There were no explicit constraints put upon us, either. And we took advantage of that. They used several mechanisms to drive adoption, but one of the most famous was Testing on the Toilet (or TotT), their weekly testing periodical. Each week, they published a newsletter in nearly every bathroom in nearly every Google office worldwide. Bland said, The goal was to raise the degree of testing knowledge and sophistication throughout the company. Its doubtful an online-only publication wouldve involved people to the same degree."
    - page: "21"
      content: "Bland continues, One of the most significant TotT episodes was the one titled, Test Certified: Lousy Name, Great Results, because it outlined two initiatives that had significant success in advancing the use of automated testing. Test Certified (TC) provided a road map to improve the state of automated testing. As Bland describes, It was intended to hack the measurement-focused priorities of Google culture...and to overcome the first, scary obstacle of not knowing where or how to start. Level 1 was to quickly establish a baseline metric, Level 2 was setting a policy and reaching an automated test coverage goal, and Level 3 was striving towards a long-term coverage goal. The second capability was providing Test Certified mentors to any team who wanted advice or help, and Test Mercenaries (i.e., a full-time team of internal coaches and consultants) to work hands-on with teams to improve their testing practices and code quality."
    - page: "21"
      content: "CONCLUSION This chapter described how we can institute rituals that help reinforce the culture that we are all lifelong learners and that we value the improvement of daily work over daily work itself. We do this by reserving time to pay down technical debt, create forums that allow everyone to learn from and teach each other, both inside our organization and outside it. And we make experts available to help internal teams, either by coaching or consulting or even just holding office hours to answer questions. By having everyone help each other learn in our daily work, we out-learn the competition, helping us win in the marketplace. But also we help each other achieve our full potential as human beings."
    - page: "21"
      content: "CONCLUSION TO PART V Throughout Part V, we explored the practices that help create a culture of learning and experimentation in your organization. Learning from incidents, creating shared repositories, and sharing learnings is essential when we work in complex systems, helping to make our work culture more just and our systems safer and more resilient. In Part VI, well explore how to extend flow, feedback, and learning and experimentation by using them to simultaneously help us achieve our Information Security goals."
    - page: "22"
      content: "Part VI:The Technological Practices Of Integrating Information Security, Change Management, And Compliance"
    - page: "22"
      content: "Introduction In the previous chapters, we discussed enabling the fast flow of work from check-in to release, as well as creating the reciprocal fast flow of feedback. We explored the cultural rituals that reinforce the acceleration of organizational learning and amplification of weak failure signals that help us create an ever safer system of work."
    - page: "22"
      content: "In Part VI, we further extend these activities so that we not only achieve Development and Operations goals, but also simultaneously achieve Information Security goals, helping us create a high degree of assurance around the confidentiality, integrity, and availability of our services and data."
    - page: "22"
      content: "Instead of inspecting security into our product at the end of the process, we will create and integrate security controls into the daily work of Development and Operations, so that security is part of everyones job, every day. Ideally, this work will be automated and put into our deployment pipeline."
    - page: "22"
      content: "By automating these activities, we can generate evidence on demand to demonstrate that our controls are operating effectively, whether to auditors, assessors, or anyone else working in our value stream. In the end, we will not only improve security, but also create processes that are easier to audit and that attest to the effectiveness of controls, in support of compliance with regulatory and contractual obligations."
    - page: "22"
      content: "We do this by: Making security a part of everyones job Integrating preventative controls into our shared source code repository Integrating security with our deployment pipeline Integrating security with our telemetry to better enable detection and recovery Protecting our deployment pipeline Integrating our deployment activities with our change approval processes Reducing reliance on separation of duty"
    - page: "22"
      content: "When we integrate security work into everyones daily work, making it everyones responsibility, we help the organization have better security. Better security means that we are defensible and sensible with our data. It means that we are reliable and have business continuity by being more available and more capable of easily recovering from issues. We are also able to overcome security problems before they cause catastrophic results, and we can increase the predictability of our systems. And, perhaps most importantly, we can secure our systems and data better than ever."
    - page: "22"
      content: "Chapter 22: Information Security as Everyones Job, Every Day"
    - page: "22"
      content: "Information Security as Everyones Job, Every Day One of the top objections to implementing DevOps principles and patterns has been, Information security and compliance wont let us. And yet, DevOps may be one of the best ways to better integrate information security into the daily work of everyone in the technology value stream."
    - page: "22"
      content: "One interpretation of DevOps is that it came from the need to enable developers productivity, because as the number of developers grew, there werent enough Ops people to handle all the resulting deployment work. This shortage is even worse in Infosec—the ratio of engineers in Development, Operations, and Infosec in a typical technology organization is 100:10:1. When Infosec is that outnumbered, without automation and integrating information security into the daily work of Dev and Ops, Infosec can only do compliance checking, which is the opposite of security engineering—and besides, it also makes everyone hate us."
    - page: "22"
      content: "INTEGRATE SECURITY INTO DEVELOPMENT ITERATION DEMONSTRATIONS One of our goals is to have feature teams engaged with Infosec as early as possible, as opposed to primarily engaging at the end of the project. One way we can do this is by inviting Infosec to the product demonstrations at the end of each development interval so that they can better understand the team goals in the context of organizational goals, observe their implementations as they are being built, and provide guidance and feedback at the earliest stages of the project, when there is the most amount of time and freedom to make corrections."
    - page: "22"
      content: "By having Infosec involved throughout the creation of any new capability, we were able to reduce our use of static checklists dramatically and rely more on using their expertise throughout the entire software development process."
    - page: "22"
      content: "This helped the organization achieve its goals. Snehal Antani, former CIO of Enterprise Architecture at GE Capital Americas, described their top three key business measurements were development velocity (i.e., speed of delivering features to market), failed customer interactions (i.e., outages, errors), and compliance response time (i.e., lead time from audit request to delivery of all quantitative and qualitative information required to fulfill the request)."
    - page: "22"
      content: "INTEGRATE SECURITY INTO DEFECT TRACKING AND POST-MORTEMS"
    - page: "22"
      content: "When possible, we want to track all open security issues in the same work tracking system that Development and Operations are using, ensuring the work is visible and can be prioritized against all other work. This is very different from how Infosec has traditionally worked, where all security vulnerabilities are stored in a GRC (governance, risk, and compliance) tool that only Infosec has access to."
    - page: "22"
      content: "Furthermore, he states, Any time we had a security issue, we would conduct a post-mortem, because it would result in better educating our engineers on how to prevent it from happening again in the future, as well as a fantastic mechanism for transferring security knowledge to our engineering teams."
    - page: "22"
      content: "INTEGRATE PREVENTIVE SECURITY CONTROLS INTO SHARED SOURCE CODE REPOSITORIES AND SHARED SERVICES"
    - page: "22"
      content: "Now we will add to our shared source code repository any mechanisms or tools that help enable us to ensure our applications and environments are secure."
    - page: "22"
      content: "We will add libraries that are pre-blessed by security to fulfill specific Infosec objectives, such as authentication and encryption libraries and services. Because everyone in the DevOps value stream uses version control for anything they build or support, putting our information security artifacts there makes it much easier to influence the daily work of Dev and Ops, because anything we create is available, searchable, and reusable."
    - page: "22"
      content: "If we have a centralized shared services organization, we may also collaborate with them to create and operate shared security-relevant platforms, such as authentication, authorization, logging, and other security and auditing services that Dev and Ops require. When engineers use one of these predefined libraries or services, they wont need to schedule a separate security design review for that module; theyll be using the guidance weve created concerning configuration hardening, database security settings, key lengths, and so forth."
    - page: "22"
      content: "Our shared repository not only becomes the place where we can get the latest versions, but also becomes a place where we can collaborate with other engineers and monitor and alert on changes made to security-sensitive modules."
    - page: "22"
      content: "INTEGRATE SECURITY INTO OUR DEPLOYMENT PIPELINE"
    - page: "22"
      content: "In previous eras, in order to harden and secure our application, we would start our security review after development was completed. Often, the output of this review would be hundreds of pages of vulnerabilities in a PDF, which wed give to Development and Operations, which would be completely un-addressed due to project due date pressure or problems being found too late in the software life cycle to be easily corrected."
    - page: "22"
      content: "Tools such as Gauntlt have been designed to integrate into the deployment pipelines, which run automated security tests on our applications, our application dependencies, our environment, etc. Remarkably, Gauntlt even puts all its security tests in Gherkin syntax test scripts, which is widely used by developers for unit and functional testing. Doing this puts security testing in a framework they are likely already familiar with. This also allows security tests to easily run in a deployment pipeline on every committed change, such as static code analysis, checking for vulnerable dependencies, or dynamic testing."
    - page: "22"
      content: "ENSURE SECURITY OF THE APPLICATION Often, Development testing focuses on the correctness of functionality, looking at positive logic flows. This type of testing is often referred to as the happy path, which validates user journeys (and sometimes alternative paths) where everything goes as expected, with no exceptions or error conditions."
    - page: "22"
      content: "On the other hand, effective QA, Infosec, and Fraud practitioners will often focus on the sad paths, which happen when things go wrong, especially in relation to security-related error conditions. (These types of security-specific conditions are often jokingly referred to as the bad paths.)"
    - page: "22"
      content: "Instead of performing these tests manually, we would ideally generate them as part of our automated unit or functional tests so that they can be run continuously in our deployment pipeline. As part of our testing, we will want to include the following: Static analysis: This is testing that we perform in a non-runtime environment, ideally in the deployment pipeline. Typically, a static analysis tool will inspect program code for all possible run-time behaviors and seek out coding flaws, back doors, and potentially malicious code (this is sometimes known as testing from the inside-out). Examples of tools include Brakeman, Code Climate, and searching for banned code functions (e.g., exec()). Dynamic analysis: As opposed to static testing, dynamic analysis consists of tests executed while a program is in operation. Dynamic tests monitor items such as system memory, functional behavior, response time, and overall performance of the system. This method (sometimes known as testing from the outside-in) is similar to the manner in which a malicious third party might interact with an application. Examples include Arachni and OWASP ZAP (Zed Attack Proxy). Some types of penetration testing can also be performed in an automated fashion and should be included as part of dynamic analysis using tools such as Nmap and Metasploit. Ideally, we should perform automated dynamic testing during the automated functional testing phase of our deployment pipeline, or even against our services while they are in production. To ensure correct security handling, tools like OWASP ZAP can be configured to attack our services through a web browser proxy and inspect the network traffic within our test harness. Dependency scanning: Another type of static testing we would normally perform at build time inside of our deployment pipeline involves inventorying all our dependencies for binaries and executables, and ensuring that these dependencies, which we often dont have control over, are free of vulnerabilities or malicious binaries. Examples include Gemnasium and bundler audit for Ruby, Maven for Java, and the OWASP Dependency-Check. Source code integrity and code signing: All developers should have their own PGP key, perhaps created and managed in a system such as keybase.io. All commits to version control should be signed­—that is straightforward to configure using the open source tools gpg and git. Furthermore, all packages created by the CI process should be signed, and their hash recorded in the centralized logging service for audit purposes."
    - page: "22"
      content: "As part of our testing, we will want to include the following: Static analysis: This is testing that we perform in a non-runtime environment, ideally in the deployment pipeline. Typically, a static analysis tool will inspect program code for all possible run-time behaviors and seek out coding flaws, back doors, and potentially malicious code (this is sometimes known as testing from the inside-out). Examples of tools include Brakeman, Code Climate, and searching for banned code functions (e.g., exec())."
    - page: "22"
      content: "Dynamic analysis: As opposed to static testing, dynamic analysis consists of tests executed while a program is in operation. Dynamic tests monitor items such as system memory, functional behavior, response time, and overall performance of the system. This method (sometimes known as testing from the outside-in) is similar to the manner in which a malicious third party might interact with an application. Examples include Arachni and OWASP ZAP (Zed Attack Proxy). Some types of penetration testing can also be performed in an automated fashion and should be included as part of dynamic analysis using tools such as Nmap and Metasploit. Ideally, we should perform automated dynamic testing during the automated functional testing phase of our deployment pipeline, or even against our services while they are in production. To ensure correct security handling, tools like OWASP ZAP can be configured to attack our services through a web browser proxy and inspect the network traffic within our test harness."
    - page: "22"
      content: "Dependency scanning: Another type of static testing we would normally perform at build time inside of our deployment pipeline involves inventorying all our dependencies for binaries and executables, and ensuring that these dependencies, which we often dont have control over, are free of vulnerabilities or malicious binaries. Examples include Gemnasium and bundler audit for Ruby, Maven for Java, and the OWASP Dependency-Check."
    - page: "22"
      content: "Source code integrity and code signing: All developers should have their own PGP key, perhaps created and managed in a system such as keybase.io. All commits to version control should be signed­—that is straightforward to configure using the open source tools gpg and git. Furthermore, all packages created by the CI process should be signed, and their hash recorded in the centralized logging service for audit purposes."
    - page: "22"
      content: "Case Study—Static Security Testing at Twitter (2009)-Twitter had many challenges due to hyper-growth. For years, the famous Fail Whale error page would be displayed when Twitter did not have sufficient capacity to keep up with user demand, showing a graphic of a whale being lifted by eight birds.Twitter also had security problems during this period. In early 2009, two serious security breaches occurred. First, in January the BarackObama Twitter account was hacked.Then in April, the Twitter administrative accounts were compromised through a brute-force dictionary attack. vulnerabilities detected The results of integrating security testing into the development process were breathtaking. Over the years, by creating fast feedback for developers when they write insecure code and showing them how to fix the vulnerabilities, Brakeman has reduced the rate of vulnerabilities found by 60%, as shown in figure 44. (The spikes are usually associated with new releases of Brakeman.)"
    - page: "22"
      content: "The Open Web Application Security Project (OWASP) is a non-profit organization focused on improving the security of software."
    - page: "22"
      content: "The DBIR found that ten vulnerabilities (i.e., CVEs) accounted for almost 97% of the exploits used in studied cardholder data breaches in 2014. Of these ten vulnerabilities, eight of them were over ten years old."
    - page: "22"
      content: "The 2015 Sonatype State of the Software Supply Chain Report further analyzed the vulnerability data from the Nexus Central Repository. In 2015, this repository provided the build artifacts for over 605,000 open source projects, servicing over seventeen billion download requests of artifacts and dependencies primarily for the Java platform, originating from 106,000 organizations."
    - page: "22"
      content: "The 2015 Sonatype State of the Software Supply Chain Report further analyzed the vulnerability data from the Nexus Central Repository. In 2015, this repository provided the build artifacts for over 605,000 open source projects, servicing over seventeen billion download requests of artifacts and dependencies primarily for the Java platform, originating from 106,000 organizations. The report included these startling findings: The typical organization relied upon 7,601 build artifacts (i.e., software suppliers or components) and used 18,614 different versions (i.e., software parts). Of those components being used, 7.5% had known vulnerabilities, with over 66% of those vulnerabilities being over two years old without having been resolved. The last statistic confirms another information security study by Dr. Dan Geer and Josh Corman, which showed that of the open source projects with known vulnerabilities registered in the National Vulnerability Database, only 41% were ever fixed and required, on average, 390 days to publish a fix. For those vulnerabilities that were labeled at the highest severity (i.e., those scored as CVSS level 10), fixes required 224 days."
    - page: "22"
      content: "ENSURE SECURITY OF THE ENVIRONMENT In this step, we should do whatever is required to help ensure that the environments are in a hardened, risk-reduced state."
    - page: "22"
      content: "Case Study—18F Automating Compliance for the Federal Government with Compliance Masonry - US Federal Government agencies were projected to spend nearly $80 billion on IT in 2016, supporting the mission of all the executive branch agencies. Regardless of agency, to take any system from dev complete to live in production requires obtaining an Authority to Operate (ATO) from a Designated Approving Authority (DAA)"
    - page: "22"
      content: "INTEGRATE INFORMATION SECURITY INTO PRODUCTION TELEMETRY"
    - page: "22"
      content: "In order to detect problematic user behavior that could be an indicator or enabler of fraud and unauthorized access, we must create the relevant telemetry in our applications. Examples may include: Successful and unsuccessful user logins User password resets User email address resets User credit card changes. For instance, as an early indicator of brute-force login attempts to gain unauthorized access, we might display the ratio of unsuccessful login attempts to successful logins."
    - page: "22"
      content: "CREATING SECURITY TELEMETRY IN OUR ENVIRONMENT In addition to instrumenting our application, we also need to create sufficient telemetry in our environments so that we can detect early indicators of unauthorized access, especially in the components that are running on infrastructure that we do not control (e.g., hosting environments, in the cloud)"
    - page: "22"
      content: "We need to monitor and potentially alert on items, including the following: OS changes (e.g., in production, in our build infrastructure) Security group changes Changes to configurations (e.g., OSSEC, Puppet, Chef, Tripwire) Cloud infrastructure changes (e.g., VPC, security groups, users and privileges) XSS attempts (i.e., cross-site scripting attacks) SQLi attempts (i.e., SQL injection attacks) Web server errors (e.g., 4XX and 5XX errors)"
    - page: "22"
      content: "Case Study—Instrumenting the Environment at Etsy (2010) - In 2010, Nick Galbreath was director of engineering at Etsy and responsible for information security, fraud control, and privacy. Galbreath defined fraud as when the system works incorrectly, allowing invalid or un-inspected input into the system, causing financial loss, data loss/theft, system downtime, vandalism, or an attack on another system."
    - page: "22"
      content: "PROTECT OUR DEPLOYMENT PIPELINE The infrastructure that supports our continuous integration and continuous deployment processes also presents a new surface area vulnerable to attack."
    - page: "22"
      content: "However, in order to protect our continuous build, integration, or deployment pipeline, our mitigation strategies may include:"
    - page: "22"
      content: "Hardening continuous build and integration servers and ensuring we can reproduce them in an automated manner, just as we would for infrastructure that supports customer-facing production services, to prevent our continuous build and integration servers from being compromised"
    - page: "22"
      content: "Reviewing all changes introduced into version control, either through pair programming at commit time or by a code review process between commit and merge into trunk, to prevent continuous integration servers from running uncontrolled code (e.g., unit tests may contain malicious code that allows or enables unauthorized access) Instrumenting our repository to detect when test code contains suspicious API calls (e.g., unit tests accessing the filesystem or network) is checked in to the repository, perhaps quarantining it and triggering an immediate code review Ensuring every CI process runs on its own isolated container or VM Ensuring the version control credentials used by the CI system are read-only"
    - page: "22"
      content: "CONCLUSION Throughout this chapter we have described ways to integrate information security objectives into all stages of our daily work. We do this by integrating security controls into the mechanisms weve already created, ensuring that all on-demand environments are also in a hardened, risk-reduced state—by integrating security testing into the deployment pipeline and ensuring the creation of security telemetry in pre-production and production environments. By doing so, we enable developer and operational productivity to increase while simultaneously increasing our overall safety. Our next step is to protect the deployment pipeline."
    - page: "23"
      content: "Chapter 23:Protecting the Deployment Pipeline"
    - page: "23"
      content: "INTEGRATE SECURITY AND COMPLIANCE INTO CHANGE APPROVAL PROCESSES"
    - page: "23"
      content: "Effective change management policies will recognize that there are different risks associated with different types of changes and that those changes are all handled differently. These processes are defined in ITIL, which breaks changes down into three categories:"
    - page: "23"
      content: "Standard changes: These are lower-risk changes that follow an established and approved process, but can also be pre-approved. They include monthly updates of application tax tables or country codes, website content and styling changes, and certain types of application or operating system patches that have a well-understood impact. The change proposer does not require approval before deploying the change, and change deployments can be completely automated and should be logged so there is traceability."
    - page: "23"
      content: "Normal changes: These are higher-risk changes that require review or approval from the agreed upon change authority. In many organizations, this responsibility is inappropriately placed on the change advisory board (CAB) or emergency change advisory board (ECAB), which may lack the required expertise to understand the full impact of the change, often leading to unacceptably long lead times. This problem is especially relevant for large code deployments, which may contain hundreds of thousands (or even millions) of lines of new code, submitted by hundreds of developers over the course of several months. In order for normal changes to be authorized, the CAB will almost certainly have a well-defined request for change (RFC) form that defines what information is required for the go/no-go decision. The RFC form usually includes the desired business outcomes, planned utility and warranty, a business case with risks and alternatives, and a proposed schedule."
    - page: "23"
      content: "Urgent changes: These are emergency, and, consequently, potentially high risk, changes that must be put into production immediately (e.g., urgent security patch, restore service). These changes often require senior management approval, but allow documentation to be performed after the fact."
    - page: "23"
      content: "A key goal of DevOps practices is to streamline our normal change process such that it is also suitable for emergency changes."
    - page: "23"
      content: "ITIL defines utility as what the service does, while warranty is defined as how the service is delivered and can be used to determine whether a service is fit for use."
- author:  Jeff Desjardins, Founder and editor of Visual Capitalist 
  title: "10 skills you'll need to survive the rise of automation"
  finished: 2018-07-07
  rating: 5
  quotes:
    - page: "1"
      content: "https://www.weforum.org/agenda/2018/07/the-skills-needed-to-survive-the-robot-invasion-of-the-workplace"    
- author:  Jeff Desjardins, Founder and editor of Visual Capitalist 
  title: "Visualizing the Massive $15.7 Trillion Impact of AI"
  finished: 2018-05-25
  rating: 3.5
  quotes:
    - page: "1"
      content: "http://www.visualcapitalist.com/economic-impact-artificial-intelligence-ai/"   
- author:  Dominic Rushe in New York 
  title: "Bitcoin and cryptocurrencies 'will come to bad end', says Warren Buffett"
  finished: 2018-03-25
  rating: 3
  quotes:
    - page: "1"
      content: "https://amp.theguardian.com/technology/2018/jan/10/bitcoin-and-cryptocurrencies-will-come-to-a-bad-end-says-warren-buffett"
